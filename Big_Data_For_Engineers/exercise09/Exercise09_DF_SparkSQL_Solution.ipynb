{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Big Data for Engineers &ndash; Exercises &ndash; Solution</center>\n",
    "## <center>Spring 2022 &ndash; Week 9 &ndash; ETH Zurich</center>\n",
    "## <center>Spark Dataframes and SparkSQL</center>\n",
    "\n",
    "# Preparation for the exercise in Spark\n",
    "\n",
    "1. Change to `exercise09` repository\n",
    "\n",
    "2. Start docker <br>\n",
    "```docker-compose up -d``` <br>\n",
    "(This process can take up to 10 minutes.)\n",
    "\n",
    "3. After docker finishes downloading the images, you should be able to start the jupyter notebook by copying the following URL to your browser <br>\n",
    "```http://127.0.0.1:8888/``` \n",
    "\n",
    "4. copy the data to docker <br> ```docker cp orders.jsonl jupyter:/home/jovyan/work``` <br>\n",
    "(Copying the data to docker needs to be done only once and it might take 1-2 minutes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>1. Spark Dataframes</center>\n",
    "\n",
    "Spark Dataframes allow the user to perform simple and efficient operations on data, as long as the data is structured and has a schema. Dataframes are similar to relational tables in relational databases: conceptually a dataframe is a specialization of a Spark RDD with schema information attached. You can find more information in Karau, H. et al. (2015). Learning Spark, Chapter 9 (optional reading).\n",
    "\n",
    "Throughout the exercise, you can see the equivalency of Spark RDD, Spark Dataframes and SparkSQL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3314.6728515625,
      "end_time": 1573739117708.542
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "path = \"orders.jsonl\"\n",
    "orders_df = spark.read.json(path).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of our dataset object is DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 35.862060546875,
      "end_time": 1573665101742.127
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.81103515625,
      "end_time": 1573665103317.247
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- price: double (nullable = true)\n",
      " |    |    |-- product: string (nullable = true)\n",
      " |    |    |-- quantity: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3283.30908203125,
      "end_time": 1573665107643.345
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customer=Row(first_name='Preston', last_name='Landry'), date='2018-2-4', items=[Row(price=1.53, product='fan', quantity=5), Row(price=1.33, product='computer screen', quantity=6), Row(price=1.06, product='kettle', quantity=6), Row(price=1.96, product='stuffed animal', quantity=3), Row(price=1.09, product='the book', quantity=7), Row(price=1.42, product='headphones', quantity=9), Row(price=1.67, product='whiskey bottle', quantity=3)], order_id=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.limit(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the underlying RDD object and use any functions you learned for Spark RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 17329.39501953125,
      "end_time": 1573665345486.969
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1960"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.rdd.filter(lambda ordr: ordr.customer.last_name == \"Landry\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dataframe Operations\n",
    "We perform some queries using operations on Dataframes ([Here](https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#untyped-dataset-operations-aka-dataframe-operations) is a guide on DF Operations with a link to the [API Documentation](https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select columns and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 252.5771484375,
      "end_time": 1573665989686.293
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|   Preston|   Landry|\n",
      "|    Jamari|Dominguez|\n",
      "|   Brendon|  Sicilia|\n",
      "|    Armani|   Ardeni|\n",
      "|    Jamari|     Miao|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.select(\"customer.first_name\", \"customer.last_name\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we can navigate to the nested items with the dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2263.60888671875,
      "end_time": 1573665774856.528
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1960"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.filter(orders_df[\"customer.last_name\"] == \"Landry\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about nested arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.12890625,
      "end_time": 1573666229796.764
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|order_id|               items|\n",
      "+--------+--------------------+\n",
      "|       0|[{1.53, fan, 5}, ...|\n",
      "|       1|[{1.61, fan, 7}, ...|\n",
      "|       2|[{1.41, the book,...|\n",
      "|       3|[{1.05, computer ...|\n",
      "|       4|[{1.92, headphone...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.select(\"order_id\", \"items\").orderBy(\"order_id\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to find orders of a fan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 239.119140625,
      "end_time": 1573666737735.271
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "orders_df.filter(orders_df[\"items.product\"] == \"fan\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code doesn't work! Use [```array_contains```](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.array_contains.html) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.64599609375,
      "end_time": 1573666726393.938
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32778"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "orders_df.filter(array_contains(\"items.product\", \"fan\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Let us try to unnest the data.</b>\n",
    "\n",
    "Unnest the products with [`explode`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.explode.html).\n",
    "\n",
    "`explode` will generate as many rows as there are elements in the array and match them to other attributes via projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1255.80712890625,
      "end_time": 1573666787807.612
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------+\n",
      "|                   i|        product|order_id|\n",
      "+--------------------+---------------+--------+\n",
      "|      {1.53, fan, 5}|            fan|       0|\n",
      "|{1.33, computer s...|computer screen|       0|\n",
      "|   {1.06, kettle, 6}|         kettle|       0|\n",
      "|{1.96, stuffed an...| stuffed animal|       0|\n",
      "| {1.09, the book, 7}|       the book|       0|\n",
      "+--------------------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\").orderBy(\"order_id\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this table to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 746.837158203125,
      "end_time": 1573667003917.751
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39922"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\")\n",
    "exploded_df.filter(exploded_df[\"product\"] == \"fan\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have tried to access the `i.product` column directly using a ```.filter``` right after the ```.select```. That, however, does not work, because the column is not available to ```orders_df``` when creating a clause like ```(orders_df[\"i.product\"] == \"fan\")```. A possible workaround when using Dataframe operations is to use a string clause in ```.filter```, so that the product column will be resolved after it has been added with the ```.select```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 247.906005859375,
      "end_time": 1573667777707.59
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39922"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\").filter(\"product = 'fan'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any ideas why there are more \"fan\" in the `explode` query than the `array_contain` one? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because that there could be more than one \"fan\" types in each order. You will find about that when inspecting the `orders.jsonl` data. \n",
    "E.g., \n",
    "```json\n",
    "{\"order_id\": 2, \"date\": \"2016-6-6\", \"customer\": {\"first_name\": \"Brendon\", \"last_name\": \"Sicilia\"}, \"items\": [..., {\"product\": \"fan\", \"quantity\": 7, \"price\": 1.1}, ..., {\"product\": \"fan\", \"quantity\": 8, \"price\": 1.15}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Project the nested columns.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 269.365966796875,
      "end_time": 1573669285846.051
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+--------+-----+---------------+--------+\n",
      "|order_id|first_name|last_name|    date|price|        product|quantity|\n",
      "+--------+----------+---------+--------+-----+---------------+--------+\n",
      "|       0|   Preston|   Landry|2018-2-4| 1.53|            fan|       5|\n",
      "|       0|   Preston|   Landry|2018-2-4| 1.33|computer screen|       6|\n",
      "|       0|   Preston|   Landry|2018-2-4| 1.06|         kettle|       6|\n",
      "+--------+----------+---------+--------+-----+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.select(explode(\"items\").alias(\"i\"), \"*\").select(\n",
    "    \"order_id\", \"customer.*\", \"date\", \"i.*\").limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Find the average quantity at which each product is purchased. Only show the top 10 products by average quantity. <br> \n",
    "(Hint: You may need to import the function ```desc``` from ```pyspark.sql.functions``` to define descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 4287.89892578125,
      "end_time": 1573675535490.617
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|        product|    avg(quantity)|\n",
      "+---------------+-----------------+\n",
      "|        toaster|5.515549016184942|\n",
      "|       the book|5.514178678641427|\n",
      "|         kettle|5.512053325314489|\n",
      "|computer screen|5.504839685420448|\n",
      "|     mouse trap|5.503895651308093|\n",
      "|            fan|5.496342868593758|\n",
      "|     headphones|5.485920795060985|\n",
      "|       notebook|5.483182341458532|\n",
      "| whiskey bottle|5.475555222463714|\n",
      "| stuffed animal|5.470854598218753|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "orders_df.select(explode(\"items\").alias(\"i\"), \"*\").select(\n",
    "    \"i.product\", \"i.quantity\"\n",
    ").groupBy(\"product\").avg(\"quantity\").orderBy(desc(\"avg(quantity)\")).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the most expensive order. <br>\n",
    "(Hint: You first build a dataframe by `explode` the items. Then you calculate the total price and aggregate per order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2290.953125,
      "end_time": 1573669705281.358
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|order_id|        sum(total)|\n",
      "+--------+------------------+\n",
      "|   99636|104.95999999999998|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"*\")\n",
    "exploded_df.select(\n",
    "    \"order_id\", (exploded_df[\"i.quantity\"] * exploded_df[\"i.price\"]).alias(\"total\")\n",
    ").groupBy(\"order_id\").sum(\"total\").orderBy(desc(\"sum(total)\")).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>2. Spark SQL</center>\n",
    "\n",
    "Spark SQL allows the users to formulate their queries using SQL. The requirement is the use of Dataframes, which as said before are similar to relational tables. In addition to a familiar interface, writing queries in SQL might provide better performance than RDDs, inheriting efficiency from the Dataframe operations, while also performing automatic optimization of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install the `sparksql` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sparksql-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use SQL we need to create a temporary table.\n",
    "\n",
    "<b>Note this table only exists for the current session.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 42.614990234375,
      "end_time": 1573668230627.757
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_df.registerTempTable(\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run SQL queries on the registered table `orders`. We will run the same queries as during the previous section, but with SQL.\n",
    "\n",
    "As you can see we can navigate to the nested items with the dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 11478.6259765625,
      "end_time": 1573665795839.541
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>1960</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- Finally, run SQL queries on the registered table \"orders\"\n",
    "-- As you can see we can navigate to the nested items with the dot\n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE orders.customer.last_name == \"Landry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about nested arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2276.55419921875,
      "end_time": 1573666251672.414
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">items</td></tr><tr><td>0</td><td>[Row(price=1.53, product=&#x27;fan&#x27;, quantity=5), Row(price=1.33, product=&#x27;computer screen&#x27;, quantity=6), Row(price=1.06, product=&#x27;kettle&#x27;, quantity=6), Row(price=1.96, product=&#x27;stuffed animal&#x27;, quantity=3), Row(price=1.09, product=&#x27;the book&#x27;, quantity=7), Row(price=1.42, product=&#x27;headphones&#x27;, quantity=9), Row(price=1.67, product=&#x27;whiskey bottle&#x27;, quantity=3)]</td></tr><tr><td>1</td><td>[Row(price=1.61, product=&#x27;fan&#x27;, quantity=7), Row(price=1.39, product=&#x27;whiskey bottle&#x27;, quantity=2)]</td></tr><tr><td>2</td><td>[Row(price=1.41, product=&#x27;the book&#x27;, quantity=6), Row(price=1.3, product=&#x27;notebook&#x27;, quantity=5), Row(price=1.1, product=&#x27;fan&#x27;, quantity=7), Row(price=1.5, product=&#x27;stuffed animal&#x27;, quantity=10), Row(price=1.39, product=&#x27;headphones&#x27;, quantity=8), Row(price=1.78, product=&#x27;whiskey bottle&#x27;, quantity=3), Row(price=1.15, product=&#x27;fan&#x27;, quantity=8)]</td></tr><tr><td>3</td><td>[Row(price=1.05, product=&#x27;computer screen&#x27;, quantity=10), Row(price=1.5, product=&#x27;stuffed animal&#x27;, quantity=10), Row(price=1.42, product=&#x27;whiskey bottle&#x27;, quantity=10)]</td></tr><tr><td>4</td><td>[Row(price=1.92, product=&#x27;headphones&#x27;, quantity=2), Row(price=1.44, product=&#x27;fan&#x27;, quantity=2), Row(price=1.84, product=&#x27;kettle&#x27;, quantity=4), Row(price=1.44, product=&#x27;stuffed animal&#x27;, quantity=5)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- How about nested arrays?\n",
    "SELECT order_id, items\n",
    "FROM orders AS o\n",
    "ORDER BY order_id\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to find orders of a fan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 248.202880859375,
      "end_time": 1573666528302.263
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql \n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE items.product = \"fan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code doesn't work! Use [```array_contains```](https://spark.apache.org/docs/latest/api/sql/index.html#array_contains) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 752.942138671875,
      "end_time": 1573666530734.473
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>32778</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "\n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE array_contains(items.product, \"fan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to unnest the data.\n",
    "\n",
    "Unnest the products with [`explode`](https://spark.apache.org/docs/latest/api/sql/index.html#explode).\n",
    "\n",
    "`explode` will generate as many rows as there are elements in the array and match them to other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 772.9169921875,
      "end_time": 1573667016192.464
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">i</td><td style=\"font-weight: bold\">product</td><td style=\"font-weight: bold\">order_id</td></tr><tr><td>Row(price=1.53, product=&#x27;fan&#x27;, quantity=5)</td><td>fan</td><td>0</td></tr><tr><td>Row(price=1.33, product=&#x27;computer screen&#x27;, quantity=6)</td><td>computer screen</td><td>0</td></tr><tr><td>Row(price=1.06, product=&#x27;kettle&#x27;, quantity=6)</td><td>kettle</td><td>0</td></tr><tr><td>Row(price=1.96, product=&#x27;stuffed animal&#x27;, quantity=3)</td><td>stuffed animal</td><td>0</td></tr><tr><td>Row(price=1.09, product=&#x27;the book&#x27;, quantity=7)</td><td>the book</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT explode(items) as i, i.product, order_id\n",
    "FROM orders\n",
    "ORDER BY order_id\n",
    "limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this table to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3281.930908203125,
      "end_time": 1573667022422.047
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>39922</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- Filter on product\n",
    "SELECT count(*)\n",
    "    FROM (\n",
    "    SELECT explode(items) as i, i.product, order_id\n",
    "    FROM orders\n",
    "    ORDER BY order_id\n",
    "    )\n",
    "WHERE product = \"fan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have tried to access the `i.product` column directly in the same ```SELECT``` clause. That, however, does not work, because the column is not available to the ```WHERE``` clause. In order to access the built columns directly, we need to unnest the data and make it part of our ```FROM``` clause. [```LATERAL VIEW```](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-lateral-view.html) lets us do just that, matching each non-array attribute to an unnested row from the array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 770.024169921875,
      "end_time": 1573667932258.994
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">customer</td><td style=\"font-weight: bold\">date</td><td style=\"font-weight: bold\">items</td><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">flat_items</td></tr><tr><td>Row(first_name=&#x27;Preston&#x27;, last_name=&#x27;Landry&#x27;)</td><td>2018-2-4</td><td>[Row(price=1.53, product=&#x27;fan&#x27;, quantity=5), Row(price=1.33, product=&#x27;computer screen&#x27;, quantity=6), Row(price=1.06, product=&#x27;kettle&#x27;, quantity=6), Row(price=1.96, product=&#x27;stuffed animal&#x27;, quantity=3), Row(price=1.09, product=&#x27;the book&#x27;, quantity=7), Row(price=1.42, product=&#x27;headphones&#x27;, quantity=9), Row(price=1.67, product=&#x27;whiskey bottle&#x27;, quantity=3)]</td><td>0</td><td>Row(price=1.53, product=&#x27;fan&#x27;, quantity=5)</td></tr><tr><td>Row(first_name=&#x27;Jamari&#x27;, last_name=&#x27;Dominguez&#x27;)</td><td>2016-1-8</td><td>[Row(price=1.61, product=&#x27;fan&#x27;, quantity=7), Row(price=1.39, product=&#x27;whiskey bottle&#x27;, quantity=2)]</td><td>1</td><td>Row(price=1.61, product=&#x27;fan&#x27;, quantity=7)</td></tr><tr><td>Row(first_name=&#x27;Brendon&#x27;, last_name=&#x27;Sicilia&#x27;)</td><td>2016-6-6</td><td>[Row(price=1.41, product=&#x27;the book&#x27;, quantity=6), Row(price=1.3, product=&#x27;notebook&#x27;, quantity=5), Row(price=1.1, product=&#x27;fan&#x27;, quantity=7), Row(price=1.5, product=&#x27;stuffed animal&#x27;, quantity=10), Row(price=1.39, product=&#x27;headphones&#x27;, quantity=8), Row(price=1.78, product=&#x27;whiskey bottle&#x27;, quantity=3), Row(price=1.15, product=&#x27;fan&#x27;, quantity=8)]</td><td>2</td><td>Row(price=1.1, product=&#x27;fan&#x27;, quantity=7)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT *\n",
    "FROM orders LATERAL VIEW explode(items) as flat_items\n",
    "WHERE flat_items.product = \"fan\"\n",
    "ORDER BY order_id\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the nested columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2275.98095703125,
      "end_time": 1573667943996.512
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">first_name</td><td style=\"font-weight: bold\">last_name</td><td style=\"font-weight: bold\">date</td><td style=\"font-weight: bold\">price</td><td style=\"font-weight: bold\">product</td><td style=\"font-weight: bold\">quantity</td></tr><tr><td>0</td><td>Preston</td><td>Landry</td><td>2018-2-4</td><td>1.53</td><td>fan</td><td>5</td></tr><tr><td>1</td><td>Jamari</td><td>Dominguez</td><td>2016-1-8</td><td>1.61</td><td>fan</td><td>7</td></tr><tr><td>2</td><td>Brendon</td><td>Sicilia</td><td>2016-6-6</td><td>1.1</td><td>fan</td><td>7</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT order_id, customer.first_name, customer.last_name, date, flat_items.*\n",
    "FROM orders LATERAL VIEW explode(items) item_table as flat_items\n",
    "WHERE flat_items.product = \"fan\"\n",
    "ORDER BY order_id\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built an unnested table, we can now easily aggregate over the previously nested columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Find the average quantity at which each product is purchased. Only show the top 10 products by quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2274.546142578125,
      "end_time": 1573669714658.905
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">product</td><td style=\"font-weight: bold\">avg_quantity</td></tr><tr><td>toaster</td><td>5.515549016184942</td></tr><tr><td>the book</td><td>5.514178678641427</td></tr><tr><td>kettle</td><td>5.512053325314489</td></tr><tr><td>computer screen</td><td>5.504839685420448</td></tr><tr><td>mouse trap</td><td>5.503895651308093</td></tr><tr><td>fan</td><td>5.496342868593758</td></tr><tr><td>headphones</td><td>5.485920795060985</td></tr><tr><td>notebook</td><td>5.483182341458532</td></tr><tr><td>whiskey bottle</td><td>5.475555222463714</td></tr><tr><td>stuffed animal</td><td>5.470854598218753</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT flat_items.product, AVG(flat_items.quantity) as avg_quantity\n",
    "FROM orders LATERAL VIEW explode(items) flat_table flat_items\n",
    "GROUP BY flat_items.product\n",
    "ORDER BY avg_quantity DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the most expensive order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1268.054931640625,
      "end_time": 1573669716818.317
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">total</td></tr><tr><td>99636</td><td>104.95999999999998</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT order_id, SUM(flat_items.quantity * flat_items.price) as total\n",
    "FROM orders LATERAL VIEW explode(items) flat_table flat_items\n",
    "GROUP BY order_id\n",
    "ORDER BY total desc\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qPak0E_rLuZ"
   },
   "source": [
    "## <center>3. Create Nestedness (Optional)</center>\n",
    "\n",
    "We've already had a look at the solution of dataframes/SparkSQL towards <b>unnesting</b> arrays by using `explode` method. For the other way round, Spark Dataframes / Spark SQL also provide ways for us to nest our data by creating arrays, especially after clauses like `groupBy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCUYQYzawGkZ"
   },
   "source": [
    "In traditional PostgreSQL, we have to use one of the aggregation functions (`max, sum, count,`...) to process the result after the `groupBy` operation. For example, for each customer (assume there are no customers with both the same first name and last name), we want to find out the number of distinct dates when they placed an order. You can fill in the queries in both Spark DataFrames and Spark SQL. The query could look like this using [`countDistinct`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.countDistinct.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 11936.587890625,
      "end_time": 1620477905132.344
     }
    },
    "id": "5FI98iZ70Ipm",
    "outputId": "70643201-9a3c-4f55-f782-58ef447d339e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------+\n",
      "|customer.first_name|customer.last_name|count(date)|\n",
      "+-------------------+------------------+-----------+\n",
      "|               Zane|              Dahl|          3|\n",
      "|             Dorian|              Dahl|          2|\n",
      "|               Rory|             Dower|          2|\n",
      "|             Morgan|              Miao|          2|\n",
      "|             Ashlyn|             Hatch|          1|\n",
      "|             Landen|         Galatioto|          2|\n",
      "|              Allan|                Po|          4|\n",
      "|           Clarissa|           Sicilia|          2|\n",
      "|              Annie|             Dower|          2|\n",
      "|              Micah|                Mo|          4|\n",
      "|             Morgan|           Poitras|          2|\n",
      "|             Gordon|            Gruber|          2|\n",
      "|         Alexandria|       Butterfield|          3|\n",
      "|             Thomas|             Dower|          1|\n",
      "|              Ariel|           Coulson|          3|\n",
      "|            Xiomara|         Christofi|          2|\n",
      "|              Rylie|              Dahl|          1|\n",
      "|             Daniel|           Schuler|          1|\n",
      "|              Chana|           Balster|          2|\n",
      "|             Azaria|        Berenguier|          2|\n",
      "+-------------------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "orders_df.groupBy(\"customer.first_name\", \"customer.last_name\").agg(countDistinct(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 14957.180908203125,
      "end_time": 1620477984175.839
     }
    },
    "colab": {
     "referenced_widgets": [
      "fd7fe3bdf4c64325a8320ff980c20670",
      "a9efa0cac1714601a48834f0af4eb3fb"
     ]
    },
    "id": "FunDHOxO1zPs",
    "outputId": "8835267b-ab38-4bc5-ce5f-32ca505001bd"
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select customer.first_name, customer.last_name, count(distinct date) from orders \n",
    "group by customer.first_name, customer.last_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsTLU8ml2ORG"
   },
   "source": [
    "But what if we are interested not only in the count of distinct dates, but the actual\n",
    "dates themselves? Luckily Spark Dataframes / Spark SQL do provide us with methods to preserve the original information of the date list. If now we would like to know for each customer, on which dates they placed an order, we shall use [`collect_set`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.collect_set.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1418.15087890625,
      "end_time": 1620478080466.68
     }
    },
    "id": "h1oGXmWt3gss",
    "outputId": "b6c68a25-93a3-4f82-e10a-845b85479b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n",
      "|customer.first_name|  customer.last_name|   collect_set(date)|\n",
      "+-------------------+--------------------+--------------------+\n",
      "|              Abbie|                Egan|[2018-4-8, 2016-3...|\n",
      "|           Abigayle|           Mc namara|[2016-4-2, 2017-6-9]|\n",
      "|            Adalynn|              Ardeni|[2018-2-2, 2018-6...|\n",
      "|               Aden|          Rosenbloom|         [2016-3-10]|\n",
      "|             Adonis|              Badash|          [2017-6-8]|\n",
      "|            Agustin|          Srivastava|         [2018-4-10]|\n",
      "|              Aiden|             Suchoff|          [2018-2-8]|\n",
      "|             Aiyana|              Landry|          [2018-2-3]|\n",
      "|             Alaina|              Gruber|[2016-3-1, 2018-5-1]|\n",
      "|             Alayna|               Mayer|         [2016-3-10]|\n",
      "|         Alexandria|         Butterfield|[2018-5-3, 2017-4...|\n",
      "|         Alexzander|              Landry|[2017-1-3, 2017-6-3]|\n",
      "|              Alice|             Balster|[2018-4-4, 2017-4-7]|\n",
      "|              Allan|                  Po|[2016-1-9, 2016-5...|\n",
      "|              Allie|          Berenguier|[2016-2-1, 2018-4...|\n",
      "|             Alyvia|              Ardeni|          [2017-4-8]|\n",
      "|              Amari|           Bridgeman|[2017-3-8, 2018-1-7]|\n",
      "|              Amiah|Fernandez cifuentes |[2016-1-8, 2017-1...|\n",
      "|           Anderson|              Zapata|          [2016-5-7]|\n",
      "|             Andres|                  Mo|          [2017-6-2]|\n",
      "+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "orders_df.groupBy(\"customer.first_name\", \"customer.last_name\").agg(collect_set(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 10117.97900390625,
      "end_time": 1620478640139.884
     }
    },
    "colab": {
     "referenced_widgets": [
      "5eca7ac4210c44e6b45b37e7cac4bf34",
      "439cfb14c8d64843871c09a1c0be3d60"
     ]
    },
    "id": "aJW1JpzQ3lEV",
    "outputId": "68d8c40e-ba42-4245-94a5-ab49ae60da27"
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select customer.first_name, customer.last_name, collect_set(date) from orders \n",
    "group by customer.first_name, customer.last_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2rTR-O78gy3"
   },
   "source": [
    "For some other cases where we want to preserve all the entries rather than the de-duplicated ones, we can use  [`collect_list`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.collect_list.html) method. For example, for each date we want to record the last names of customers. Since two customers might share the same last name, we need to collect all of them. The query should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 12271.528076171875,
      "end_time": 1620480910312.875
     }
    },
    "id": "X_PB-uAH9I5M",
    "outputId": "f76c10ee-f085-4121-87af-0a0846061efb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------+\n",
      "|     date|collect_list(customer.last_name)|\n",
      "+---------+--------------------------------+\n",
      "| 2017-4-8|            [Egan, Gruber, Le...|\n",
      "| 2016-2-5|            [Dahl, Findley, M...|\n",
      "| 2016-1-1|            [Suchoff, Lowe, D...|\n",
      "| 2018-2-6|            [Gottardo, Po, Go...|\n",
      "| 2018-1-7|            [Macmahon, Hirsch...|\n",
      "| 2016-6-7|            [Horah, Whitla, H...|\n",
      "|2018-5-10|            [Gruber, Drago, S...|\n",
      "| 2017-5-4|            [Srivastava, Ho-s...|\n",
      "| 2018-4-7|            [Drago, Mayer, La...|\n",
      "|2018-6-10|            [Badash, Decaro, ...|\n",
      "| 2017-3-3|            [Badash, Marinko,...|\n",
      "| 2017-3-5|            [Rosenbloom, Bals...|\n",
      "| 2017-6-4|            [Whitla, Egan, Lo...|\n",
      "| 2018-6-3|            [Cerda, Berenguie...|\n",
      "| 2018-1-6|            [Zapata, Miao, Ne...|\n",
      "| 2016-4-9|            [Badash, Dahlsted...|\n",
      "| 2016-1-5|            [Suchoff, Srivast...|\n",
      "|2018-1-10|            [Srivastava, Domi...|\n",
      "| 2017-1-5|            [Dower, Zapata, M...|\n",
      "| 2018-1-4|            [Dower, Miao, Mc ...|\n",
      "+---------+--------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "orders_df.groupBy(\"date\").agg(collect_list(\"customer.last_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 30449.055908203125,
      "end_time": 1620481513099.958
     }
    },
    "colab": {
     "referenced_widgets": [
      "a2f4e8e80a2e424ba0e552e89b16a8a7",
      "2b9d668dbcfa4a639a140d117b5e3902"
     ]
    },
    "id": "z7DSOsKC9dmF",
    "outputId": "eae40ebd-63f9-4ec1-9742-f1bb391d1071",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select date, collect_list(customer.last_name) from orders group by date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoJbUmwrQOmy"
   },
   "source": [
    "Now try it on yourself.\n",
    "\n",
    "For every order on 2016-1-1, return the list of products that appeared in that order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 90.6279296875,
      "end_time": 1620480224562.293
     }
    },
    "id": "GpWf6W_ZPg9-",
    "outputId": "59ca0ced-a3e0-46a2-a010-445e5efc372a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|order_id|collect_list(i.product)|\n",
      "+--------+-----------------------+\n",
      "|    8484|   [headphones, whis...|\n",
      "|   33209|   [notebook, the bo...|\n",
      "|   84024|   [computer screen,...|\n",
      "|   91703|   [notebook, stuffe...|\n",
      "|   28555|   [kettle, computer...|\n",
      "|    3120|   [whiskey bottle, ...|\n",
      "|   48533|               [kettle]|\n",
      "|   97472|   [toaster, the boo...|\n",
      "|    1280|   [whiskey bottle, ...|\n",
      "|   74743|           [mouse trap]|\n",
      "|   85793|   [whiskey bottle, ...|\n",
      "|   23754|   [the book, the bo...|\n",
      "|   24308|   [fan, fan, whiske...|\n",
      "|    9418|           [mouse trap]|\n",
      "|   98787|   [kettle, stuffed ...|\n",
      "|   35723|   [computer screen,...|\n",
      "|   47083|   [mouse trap, fan,...|\n",
      "|   58037|   [fan, kettle, mou...|\n",
      "|   66103|           [headphones]|\n",
      "|   94704|   [notebook, notebo...|\n",
      "+--------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"*\")\n",
    "exploded_df.filter(exploded_df[\"date\"] == \"2016-1-1\").groupBy(\"order_id\").agg(collect_list(\"i.product\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ifIhGcEQmrT"
   },
   "source": [
    "For every product, return the set of dates when it's purchased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 996.149169921875,
      "end_time": 1620480623060.309
     }
    },
    "id": "ZEmGI5GUPg9-",
    "outputId": "dafebd33-ad91-4566-95cf-2942cbd25ba3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|      i.product|   collect_set(date)|\n",
      "+---------------+--------------------+\n",
      "|       the book|[2018-4-6, 2018-4...|\n",
      "|     mouse trap|[2018-4-6, 2018-4...|\n",
      "|computer screen|[2018-4-6, 2018-4...|\n",
      "| whiskey bottle|[2018-4-6, 2018-4...|\n",
      "|        toaster|[2018-4-6, 2018-4...|\n",
      "| stuffed animal|[2018-4-6, 2018-4...|\n",
      "|         kettle|[2018-4-6, 2018-4...|\n",
      "|            fan|[2018-4-6, 2018-4...|\n",
      "|     headphones|[2018-4-6, 2018-4...|\n",
      "|       notebook|[2018-4-6, 2018-4...|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "exploded_df.orderBy(\"date\").groupBy(\"i.product\").agg(collect_set(\"date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uboC94LIRThc"
   },
   "source": [
    "One of the drawbacks of the <font face=\"courier\">collect_set/collect_list</font> method is they only accept one column as the argument. Later we will see how we can create nestedness on pretty much everything after we get the hang of the mighty JSONiq."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
