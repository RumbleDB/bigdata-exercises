{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for the moodle exercise in Spark\n",
    "\n",
    "In this jupyter notebook we are going to make the preprocessing part of the dataset that is going to be used in the graded exercise of this week.\n",
    "1. Change to exercise08 repository\n",
    "\n",
    "2. Start docker <br>\n",
    "docker-compose up -d\n",
    "\n",
    "3. Getting the data:\n",
    "Follow the procedure that is described below. The dataset can be found here: http://data.greatlanguagegame.com.s3.amazonaws.com/confusion-2014-03-02.tbz2. More specifically do the following:\n",
    "- download the data      :<br> ```wget http://data.greatlanguagegame.com.s3.amazonaws.com/confusion-2014-03-02.tbz2```\n",
    "- extract the data       :<br> ```tar -jxvf confusion-2014-03-02.tbz2```\n",
    "\n",
    "4. Go into the folder of the dataset:  ```cd confusion-2014-03-02```\n",
    "\n",
    "5. Extract the part of the dataset that we will work with in this exercise: ```head -n 3000000 confusion-2014-03-02.json > confusion-part.json```\n",
    "\n",
    "\n",
    "For more information about the dataset, you can refer to https://lars.yencken.org/datasets/great-language-game/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing commands\n",
    "In your newly created notebook run these commands in order to have the dataset into an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "path = \"confusion-2014-03-02/confusion-part.json\"\n",
    "raw_data = sc.textFile(path)\n",
    "dataset = raw_data.map(json.loads).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that you will be able to run the queries of the moodle question of this week. The RDD that you have to perform your queries on is the ```dataset``` one. For example, the following command returns one element of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'guess': 'Norwegian',\n",
       "  'target': 'Norwegian',\n",
       "  'country': 'AU',\n",
       "  'choices': ['Maori', 'Mandarin', 'Norwegian', 'Tongan'],\n",
       "  'sample': '48f9c924e0d98c959d8a6f1862b3ce9a',\n",
       "  'date': '2013-08-19'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
