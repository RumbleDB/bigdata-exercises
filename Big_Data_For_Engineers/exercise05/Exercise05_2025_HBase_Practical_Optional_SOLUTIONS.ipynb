{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <center>Big Data for Engineers &ndash; Exercises &ndash; Solution</center>\n",
    "## <center>Spring 2025 &ndash; Week 5 &ndash; ETH Zurich</center>\n",
    "## <center>Wide Column Stores - Practical HBase (Optional)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will:\n",
    "* Create your own HBase cluster, fill it with data, and run basic queries.\n",
    "* Get hands-on practice with HBase using the Wikipedia dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Important note: If you are using a ARM based Mac, do the following steps before running docker compose up -d:</font>\n",
    "1. delete the original `docker-compose.yml` file\n",
    "2. rename  `docker-compose-aarch64.yml` to `docker-compose.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 &mdash; Creating and using an HBase cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to touch HBase! You will create, fill with data, and query an HBase cluster running on Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1: set up your HBase cluster using Docker:\n",
    "\n",
    "\n",
    "0. Please first get the folder from GitHub containing all the necessary files that configure Docker. Please also open Docker, you should see that docker is active with green on the UI of Docker desktop. <br>\n",
    "\n",
    "\n",
    "1. In the command line, navigate into your exercise05 folder using 'cd' command and instantiate the cluster by running:\n",
    "<br>**`docker-compose up -d`**<br>\n",
    "Note that for the first time, it might take a little bit longer to set things up, as docker might need to pull the new images.<br>\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/QvZiRalzKJEfWO1/download\">\n",
    "\n",
    "\n",
    "2. List the container names with <br>**`docker ps --format \"{{.Names}}\"`**<br>\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/D6CB7YZGhgiVfqQ/download\"><br>\n",
    "The names might slightly differ from system to system, depending on the setup. In this case, the master container is called <b><i>exercise05-hbase-master-1</i></b>. We will be using this to access the cluster, if the container is called something different, just change the name in the following commands accordingly :)\n",
    "\n",
    "\n",
    "3. Copy the <b>.csv</b> data file from your local system to the docker container (the hbase-master) where hbase is running using the command:\n",
    "<br>**`docker cp enwiki-20200920-pages-articles-multistream_small.csv exercise05-hbase-master-1:/`**<br>\n",
    "This will be used in Exercise 2.<br>\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/B0meXjOOIxKEdw7/download\"><br>\n",
    "\n",
    "\n",
    "4. Access your container's bash by running the following command:\n",
    "<br>**`docker exec -it exercise05-hbase-master-1 /bin/bash`**<br>\n",
    "Once in the bash, you can list your files and check the presence of the .csv data file:<br>\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/3YixXc2EnpGc6jp/download\"><br>\n",
    "\n",
    "\n",
    "5. We will use the <b>.csv</b> file later to populate our database. For now, let's explore the basics of HBase in this playground. To start, run the following command in the container's bash:\n",
    "<br>**`hbase shell`**<br>\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/cBPxT5ea5iswqC3/download\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: in case of port errors for Window machines\n",
    "The ports > 50000 are blocked on Windows for non-admin users. To get around this, you can either run docker-compose as admin or you can modify the docker-compose.yml file in HBase folder as follows:\n",
    "<pre>\n",
    "  namenode: \n",
    "    ...\n",
    "    ports:\n",
    "       - 40070:50070\n",
    "       \n",
    "  datanode:\n",
    "    ...\n",
    "    ports:\n",
    "       - 40075:50075\n",
    "<pre/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Interact with your HBase cluster using the shell\n",
    "\n",
    "Now we will go through some basic HBase commands, in preparation for the exercise 2, where we will import a dataset from a .csv file and run queries against it. For now, we will populate HBase by hand using simple commands which were mentioned in the lecture.\n",
    "\n",
    "Makse sure you have opened the HBase shell by running the following command:\n",
    "\n",
    "**`hbase shell`**\n",
    "\n",
    "Let's say we want to create an HBase table that will store sentences adhering to the structure <a href=\"https://en.wikipedia.org/wiki/Subject%E2%80%93verb%E2%80%93object_word_order\"><b>subject-verb-object</b></a> (e.g., \"I eat mangoes\", \"She writes books\") in different languages. Here is a schema that we may use:\n",
    "\n",
    "Table name = `sentences`\n",
    "* Column family: `words`\n",
    "  * column: `subject`\n",
    "  * column: `verb`\n",
    "  * column: `object`\n",
    "* Column family: `info`\n",
    "  * column: `language`\n",
    "\n",
    "With the following command we can create such a table (a description of HBase shell commands is available [here](https://learnhbase.wordpress.com/2013/03/02/hbase-shell-commands/)):\n",
    "\n",
    "**`create 'sentences', 'words', 'info'`**\n",
    "\n",
    "You can see the schema of the table with this command:\n",
    "\n",
    "**`describe 'sentences'`**\n",
    "\n",
    "Let's insert some sentences into our table. We will put data cell by cell using the following command format <br>\n",
    "    `put <table>, <rowId>, <columnFamily:columnQualifier>, <value>`:\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:subject', 'I'`**\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:verb', 'drink'`**\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:object', 'coffee'`**\n",
    "\n",
    "Now, let's try to query this sentence from the table:\n",
    "\n",
    "**`get 'sentences', 'row1'`**\n",
    "\n",
    "You should see output similar to this:\n",
    "\n",
    "```\n",
    "COLUMN                                   CELL\n",
    " words:object                            timestamp=1602785046682, value=coffee\n",
    " words:subject                           timestamp=1602785045625, value=I\n",
    " words:verb                              timestamp=1602785045849, value=drink\n",
    "3 row(s) in 0.0910 seconds\n",
    "```\n",
    "\n",
    "As you can see, HBase shell returns data as key-value pairs rather than as rows literally. You may also notice that the lines are lexicographically sorted by the key, which is why \"subject\" appears after \"object\" in the list.\n",
    "\n",
    "I don't know about you, but even though I love coffee, I already had too much caffeine. Let us update our sentence...\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:object', 'tea'`**\n",
    "\n",
    "As you can see, we are using the same `put` command to *update* a cell. Remember that usually HBase does not actually update cells in place&mdash;it just inserts new versions instead. If you now run the query again, you will see the new data:\n",
    "\n",
    "**`get 'sentences', 'row1'`**\n",
    "\n",
    "returns:\n",
    "\n",
    "```\n",
    "COLUMN                                   CELL\n",
    " words:object                            timestamp=1602785160890, value=tea\n",
    " words:subject                           timestamp=1602785045625, value=I\n",
    " words:verb                              timestamp=1602785045849, value=drink\n",
    "3 row(s) in 0.0200 seconds\n",
    "```\n",
    "\n",
    "We actually wanted to store sentences in different languages, so let's first set the language for the existing one:\n",
    "\n",
    "**`put 'sentences', 'row1', 'info:language', 'English'`**\n",
    "\n",
    "Note that we are now inserting a value into a different column family but for the same row. Verify with a `get` that this took effect. \n",
    "\n",
    "Now, let's add a sentence in another language (note that we are using another rowID now&mdash;`row2`):\n",
    "\n",
    "**`put 'sentences', 'row2', 'words:subject', 'Ich'`**\n",
    "\n",
    "**`put 'sentences', 'row2', 'words:verb', 'trinke'`**\n",
    "\n",
    "**`put 'sentences', 'row2', 'words:object', 'Tee'`**\n",
    "\n",
    "**`put 'sentences', 'row2', 'info:language', 'Deutsch'`**\n",
    "\n",
    "Let's check that we indeed have 2 rows now:\n",
    "\n",
    "**`count 'sentences'`**\n",
    "\n",
    "Now, let's query all rows from the table:\n",
    "\n",
    "**`scan 'sentences'`**\n",
    "\n",
    "This, indeed, returns all two rows, in key-value format as before.\n",
    "\n",
    "\n",
    "Of course, you can also scan by column family for column:\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'info'}`**\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'info:language'}`**\n",
    "\n",
    "You can also scan by row ranges (note min incl., max excl.):\n",
    "\n",
    "**`scan 'sentences', {STARTROW=>'row1', ENDROW=>'row3'}`**\n",
    "\n",
    "It is possible to also do some filtering in queries:\n",
    "\n",
    "**`scan 'sentences', {FILTER => \"ValueFilter(=, 'binary:English')\"}`**\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'words:subject', FILTER => \"ValueFilter(=, 'substring:I')\"}`**\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'words:object', ROWPREFIXFILTER => 'row'}`**\n",
    "\n",
    "What if we want to store a sentence that also contains an adjective, in addition to the subject, verb, and object? <br>\n",
    "This is not a problem with HBase, because we can create new columns inside *existing* column families on the fly:\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:subject', 'Grandma'`**\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:verb', 'bakes'`**\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:adjective', 'delicious'`**\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:object', 'cakes'`**\n",
    "\n",
    "This row now has more columns in the `words` column family than others:\n",
    "\n",
    "**`get 'sentences', 'row3'`**\n",
    "\n",
    "**`scan 'sentences'`**\n",
    "\n",
    "We can also add new columns to existing rows:\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:adjective', 'hot'`**\n",
    "\n",
    "**`get 'sentences', 'row1'`**\n",
    "\n",
    "Let's see what happens if you update a value in an existing column:\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:adjective', 'cold'`**\n",
    "\n",
    "You should notice that the time stamp of the column `words:adjective` has been updated.\n",
    "\n",
    "When you are done with the queries, simply type `exit` to quit the hbase shell.\n",
    "\n",
    "Note: to drop a table in HBase, first `disable 'table_name'`, then `drop 'table_name'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 &mdash; The Wikipedia dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will see how HBase handles a large dataset, as well as learn about the filters and caching in HBase.\n",
    "\n",
    "The provided dataset comprises of metadata information of articles from the English Wikipedia. You will see the following variables in the .csv file: \n",
    "\n",
    "| Variable | Meaning | Sample value|\n",
    "|:------|:---------------|:---------------|\n",
    "|`page_id`| the page id in the enwiki data dump |1000108|\n",
    "|`page_title`| the page title|AEG Z.6|\n",
    "|`page_ns`| page namespace|0|\n",
    "|`revision_id`| the id of revision to the article |16782282|\n",
    "|`timestamp`| the time a contributor makes a revision |2004-09-19T23:44:33Z|\n",
    "|`contributor_id`| the id of contributor |8817|\n",
    "|`contributor_name`| the name of contributor |Rlandmann|\n",
    "|`bytes`| bytes in revision |21|\n",
    "\n",
    "\n",
    "We use the `wiki_small` dataset (about 85MB in .csv) in this assignment because it takes a shorter time (about 1-5 minutes) to load into HBase.\n",
    "\n",
    "Based on the variable description in the table above, we can categorize the variables into 2 categories: <br>\n",
    "(1) some variables are about a page;<br>\n",
    "(2) some variables are about an author/contributor. <br>\n",
    "\n",
    "Let us now create the schema in HBase with two column families, `page` and `author`:\n",
    "\n",
    "**`hbase shell`**\n",
    "\n",
    "**`create 'wiki_small', 'page', 'author'`**\n",
    "\n",
    "After the table is created, we need to exit the HBase shell and return back to the container's bash:\n",
    "\n",
    "**`exit`**\n",
    "\n",
    "Now we need to populate both tables with data. We will use the [ImportTsv](https://hbase.apache.org/book.html#importtsv) utility of HBase. Populate the table `wiki_small` by running the following (keep in mind that you should run this command in the container's bash):\n",
    "\n",
    "**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY,page:page_title,page:page_ns,page:revision_id,author:timestamp,author:contributor_id,author:contributor_name,page:bytes\" wiki_small enwiki-20200920-pages-articles-multistream_small.csv`**\n",
    "\n",
    "We need to specify which column in the csv maps to which column in the HBase table. Note that we make `page_id` into the `HBASE_ROW_KEY` and how we specify the mappings between the **.csv columns** and the **family:column** in the HBase table.\n",
    "\n",
    "These commands print a lot of messages, but they are mostly informational with occasional non-critical warnings; unless something goes wrong, of course :). The commands will also report some \"Bad Lines\", but you can safely ignore this -- some lines may contain illegal characters and be dropped, but most of the data is in good shape.\n",
    "\n",
    "\n",
    "You can count how many rows there are using this command from your head node's shell: <br>\n",
    "**`hbase org.apache.hadoop.hbase.mapreduce.RowCounter 'wiki_small'`**<br>\n",
    "If everything goes right, you should see `ROWS=887784` in the output.\n",
    "\n",
    "Now let's go into HBase shell again (by running `hbase shell`) and run some queries against the `wiki_small` table. We will look at some of the filters listed by HBase if you run `show_filters` in an HBase shell, e.g., `PrefixFilter(), ValueFilter(), SingleColumnValueFilter()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Indexing \n",
    "\n",
    "How does HBase index the row keys? Recall that we choose `page_id` in the original table to be the row keys in the HBase table. Run these two queries and observe the output. What can we say about row key indexing based on their results?) <i>(Hint: !gniredro eht tuoba kniht)</i><br>\n",
    "\n",
    "  **`scan 'wiki_small', {STARTROW=>'100009', ENDROW=>'100011'}`**\n",
    "\n",
    "  **`scan 'wiki_small', {STARTROW=>'100015', ENDROW=>'100016'}`**\n",
    "\n",
    "\n",
    "## Task 2.2: Querying the table `wiki_small`\n",
    "\n",
    "Now we will make more queries. An example of such is listing the ids and titles for pages that have \"Empire\" in their title:  \n",
    "\n",
    "`scan 'wiki_small', {COLUMNS => ['page:page_id','page:page_title'], FILTER => \" (SingleColumnValueFilter ('page', 'page_title', =, 'substring:Empire') \"}`\n",
    "\n",
    "Another example would be this query which returns all values from pages that have \"Empire\" in their title and were authored by someone with \"tom\" in their name:\n",
    "\n",
    "`scan 'wiki_small', {FILTER => \" (SingleColumnValueFilter ('page', 'page_title', =, 'substring:Empire')) AND (SingleColumnValueFilter ('author', 'contributor_name', =, 'substring:tom')) \"}`\n",
    "\n",
    "You can refer to [this page](https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hbase_filtering.html) to get help with the filters needed.\n",
    "\n",
    "**Write the following queries:**\n",
    "\n",
    "  1. Select all article titles and author names where the row name starts with '`1977`'.\n",
    "  2. Select all article titles and author names where the author contains the substring '`tom`'. \n",
    "  3. Return the number of articles from 2017.\n",
    "  4. Return the number of articles created by the author '`Adam Bishop`'. (note that in the data the names have a leading and trailing |)\n",
    "  5. Return the number of articles that contain the word '`Sydney`' in the title. Think about different ways to formulate this query. \n",
    "  5. After you are done, execute your queries again, and compare the results. What changes, and why? <i>(Hint: !semit noitucexe yreuq eht evresbO)</i>\n",
    "\n",
    "## Task 2.3  Comparison\n",
    "\n",
    "How do the technologies we have seen so far compare? Think about the following questions, and answer them in your own words! \n",
    "\n",
    "1. What are the advantages and disadvantages of object stores, such as Amazon S3?\n",
    "2. What are the advantages and disadvantages of distributed File systems, e.g. HDFS?\n",
    "3. What are the advantages and disadvantages of wide column stores, such as HBase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### 2.1 Indexing\n",
    "**`scan 'wiki_small', {STARTROW=>'100009', ENDROW=>'100011'}`** returns five keys `1000092, 1000102, 1000104, 1000106, 1000108`, <br>\n",
    "**`scan 'wiki_small', {STARTROW=>'100015', ENDROW=>'100016'}`** returns seven keys `100015, 1000151, 1000153, 1000155, 1000156, 1000158, 1000159`. <br>\n",
    "Rows are sorted lexicographically by row key, in our case 'page_id'.\n",
    "You can use online <a href=\"https://tools.knowledgewalls.com/online-lexicographic-ascending-or-descending-sort\">lexicograpgically tools </a> to see how the row keys are lexicographically sorted. For us, lexicographical ordering feels intuitive for text, but it is generally uncommon for numbers. So depending on your usecase you may have to pad keys to get the sorting order you really want (without padding 10 is lexicographically smaller than 2). \n",
    "\n",
    "\n",
    "### 2.2 Queries\n",
    "  1. All article titles and author names where the row name starts with '1977':\n",
    "    \n",
    "    `scan 'wiki_small', {COLUMNS => ['page:page_title','author:contributor_name'], ROWPREFIXFILTER => '1977'}`\n",
    "       \n",
    "       \n",
    "  2. All article titles and author names where the author contains the substring 'tom':\n",
    "  \n",
    "    `scan 'wiki_small', {COLUMNS => ['page:page_title','author:contributor_name'], FILTER => \"SingleColumnValueFilter('author', 'contributor_name', =, 'substring:tom')\"}`\n",
    "    \n",
    "    \n",
    "3. Number of articles from 2017.\n",
    "    \n",
    "    `scan 'wiki_small', {COLUMNS =>'author:timestamp', FILTER => \"ValueFilter(=, 'substring:2017')\"}`\n",
    "      \n",
    "\n",
    "4. Return the number of articles created by the author 'Adam Bishop':\n",
    "\n",
    "    `scan 'wiki_small', {FILTER => \"SingleColumnValueFilter('author', 'contributor_name', =, 'binary:|Adam Bishop|')\"}`\n",
    "    \n",
    "    \n",
    "5. Return the number of articles that contain the word `Sydney` in the title.\n",
    "    \n",
    "    `scan 'wiki_small', {COLUMNS =>'page:page_title', FILTER => \"ValueFilter(=, 'substring:Sydney')\"}`\n",
    "      \n",
    "    Another way of formulating the query is using SingleColumnValueFilter\n",
    "      \n",
    "    `scan 'wiki_small', {FILTER => \" SingleColumnValueFilter ('page', 'page_title', =, 'substring:Sydney') \"}` \n",
    "\n",
    "6. After you are done, execute your queries again, and compare the results. What changes, and why?<br>There are two main points to note here:\n",
    "    1. Subsequent invocations of the same query take less time due to caching.\n",
    "    2. Queries with `ROWPREFIXFILTER` should be quicker, because the filter is applied to the already sorted row key rather than to the contents of columns. \n",
    "  \n",
    "  \n",
    "### 2.3 Comparison\n",
    "\n",
    "Here are some points that can be considered: \n",
    "\n",
    "1. **Object Store**: \n",
    "    1. Advantages:\n",
    "        1. They can store very large amounts of unstructured data \n",
    "        2. They are highly scalable and easily accessible all over the world\n",
    "    2. Disadvantages:\n",
    "        1. The data model is too simple for some use cases (black box objects) \n",
    "        2. Performance lags behind compared to DFSs or wide column stores \n",
    "        \n",
    "1. **DFS**: \n",
    "    1. Advantages:\n",
    "        1. They can store huge files even exceeding the capacity of a single machine due to breaking them into blocks\n",
    "        2. Due to distribution and replication, these systems are robust and tolerate failures.\n",
    "    2. Disadvantages:\n",
    "        1. They are not designed to be fit for smaller files \n",
    "        \n",
    "1. **Wide column store**: \n",
    "    1. Advantages:\n",
    "        1. Column families: columns frequently accessed together can be colocated, and very wide columns (affecting scan speed) can be isolated into separate families\n",
    "        2. Suitable for small to medium-sized data, indexing helps with latency\n",
    "        3. Rich and flexible data model allowing for complex querying and inserting/deleting columns on the fly. \n",
    "    2. Disadvantages:\n",
    "        1. Some storage overhead (column names stored for each row)\n",
    "        2. Complexity of use: setting up these systems and querying the data can be more tricky compared to other technologies. The user needs to carefully think about how column families row indexes are chosen to achieve good performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
