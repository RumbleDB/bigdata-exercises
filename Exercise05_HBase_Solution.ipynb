{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <center>Big Data &ndash; Exercises &ndash; Solution</center>\n",
    "## <center>Fall 2020 &ndash; Week 5 &ndash; ETH Zurich</center>\n",
    "## <center>HBase</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will consist of five main parts: \n",
    "* Hands-on practice with your own HBase cluster running in Azure\n",
    "* Architecture of HBase\n",
    "* Using HBase through Apache Hive\n",
    "* Bloom filter\n",
    "* Log-structured merge-tree (LSM tree, optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 &mdash; Creating and using an HBase cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to touch HBase! You will create, fill with data, and query an HBase cluster running on Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the following to set up an HBase cluster:\n",
    "\n",
    "**Important:** we want you to use a small but real cluster for running HBase rather than a single machine. But, these clusters burn Azure credit very quickly&mdash;the cheapest configuration consumes roughly **1 CHF per hour, which is a lot relative to your overall credit**&mdash;so it is very important for you to <font color=\"red\">**DELETE**</font> your cluster once you are done. Luckily, it is possible to keep your data intact when you delete a cluster, and see it again when you recreate it; we will touch upon this in the process. Now, let's start.\n",
    "\n",
    "0. Please first make sure you accept our lab assignment invitation sent to the email address you have been using for Azure. And configure the subscription in Azure so that you can see the HBase credit assignment under the Subscription name.<br>\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/0.Subscription_conf.png\"  style=\"width:800px;\"><br>\n",
    "\n",
    "1. In Azure portal click the \"+ Create a Resource\" button on the left, type \"hdinsight\" in the search box, and select \"Azure HDInsight\". Go ahead and click on \"Create\". HDInsight is Microsoft's cloud service which wraps Hadoop, HBase, Spark and other Big Data technologies; read more [here](https://azure.microsoft.com/en-us/services/hdinsight/).<br>\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/1.Azure_Hbase_1.png\" style=\"width:500px;\"><br>\n",
    "\n",
    "2. Azure requires users to register their subscriptions to enable the usage of \"Microsoft.HDInsight\" resource. Note that to get to the registration page, you need to try to create an HBase cluster. Then you will see the \"This subscription is not registered for HDInsight...\" message. Make sure you click to the registration page and search for \"hdinsight\" and register. It takes less than 1 minute to finish registration and refresh both in Azure and your browser to confirm that the registration has been done. <br>\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/2.HDInsight_registration.png\" style=\"width:600px;\">\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/2.HDInsight_registration_0.png\" style=\"width:700px;\">\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/2.HDInsight_registration_1.png\" style=\"width:700px;\"><br>\n",
    "\n",
    "3. Now refresh the page of \"Create HDInsight cluster\" and you will be able to use the Week 5 subscription. Choose the Resource group, Cluster name, Region, Cluster type as indicated in the screenshot and set your login password. <br>\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/3.Azure_HBase.png\" style=\"width:500px;\"> <br>\n",
    "\n",
    "4. The canonical way would be to use an HDFS cluster as a storage layer for an HBase cluster, but we will be using the Blob service of Windows Azure Storage for this purpose. This has a significant advantage of allowing you to delete your HBase cluster without losing the data: you can recreate the cluster using the same Azure Storage Account and the same container and you will see the same data. This is useful, for example, if you don't have time to finish this exercise in one sitting: you can just delete your cluster, recreate it later, and continue your work. \n",
    "Azure storage is selected by default (see the screenshot). If you go to the \"Storage\" tag, under \"Primary storage account\", Azure creates already a storage account and assigns a container to it. \n",
    "**Important: if you are recreating your HBase cluster and want to see the existing data, also choose the cluster in the same region as last time, then choose \"Select existing\" and set the container name to the one that you see in the \"Storage Accounts\" tab of Azure&mdash;by default Azure generates a new container name every time you create a cluster, which then points to a different container.** Leave everything else as it is and click \"Next\".<br>\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/3.Azure_HBase_storage.png\" style=\"width:500px;\"/><br>\n",
    "\n",
    "\n",
    "5. Now we need to choose the configuration of the nodes in our HBase cluster. It will be enough to have only 2 RegionServers (see the screenshot). As for the node size, let us be wise and select the economical option: for Head node choose \"E2 V3\"; for Zookeeper node choose \"D1 v2\"; for Region node choose \"E2 V3\". Zookeeper is a [distributed coordination service](http://zookeeper.apache.org/) used by HBase. Click \"Review+Create\".<br>\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/4.Azure_HBase_conf.png\" style=\"width:500px;\"/><br>\n",
    "\n",
    "6. In the last step, \"Summary\", check if the settings are as you intend. These clusters are expensive, so it is worth checking the price estimate at this step: for me it is 0.96 CHF/hour; if your price is larger than this, check your node sizes and counts. When done, initiate the cluster creation by clicking \"Create\". The process will take time, around 15&mdash;25 minutes; in my own case it took 15 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing your cluster\n",
    "The standard way to talk to an HBase cluster is via the Java API, you can find an overview [here](https://hbase.apache.org/apidocs/). There are also API in other programming languages. But in this exercise, we will interact with the HBase cluster through the command-line interface. For this, you will need to run the `ssh` program in a terminal in order to connect to your cluster. There are three options of how you can do this:\n",
    "1. **On your own machine** you can just use a normal terminal if you have `ssh` installed. Linux usually has it, as does MacOS. Windows doesn't have it by default (perhaps Win10 does, though), but Windows users can use the browser-based option, which is described next, and the other option is to install [PuTTY](http://www.putty.org/).\n",
    "\n",
    "1. Use the **Azure Cloud Shell** in your browser. Click on the Cloud Shell icon at the top of Azure Dashboard toolbar:\n",
    "    ![](https://bigdata2020exassets.blob.core.windows.net/ex05/Azure_terminal.png)<br>\n",
    "\n",
    "   You are asked to create a storage associated with the shell, please choose the Week 5 subscription for this. Note the Azure cloud shell has a time-out every 20-30 minutes, it is not suited for jobs that run longer. \n",
    "    <img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/shell_storage.png\" style=\"width:500px;\"/><br>\n",
    "\n",
    "In your terminal of choice, run the following: `ssh <ssh_user_name>@<cluster_name>-ssh.azurehdinsight.net`. In this command, `<ssh_user_name>` is the \"ssh username\" that you have chosen in the first step of creating the HBase cluster, and `<cluster_name>` also comes from that form. Note that the cluster name has to be suffixed with `-ssh`. \n",
    "\n",
    "\n",
    "This command with everything filled-in is also available on the Azure page of your HBase cluster, if you click \"SSH\" + Cluster login\". Copy the SSH from there:\n",
    "\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/ssh.png\" style=\"width:600px;\"/><br>\n",
    "\n",
    "\n",
    "\n",
    "If after running the `ssh` command you see a message similar to this:\n",
    "```\n",
    "Welcome to HBase on HDInsight.\n",
    "\n",
    "Last login: Thu Oct 15 17:51:43 2020 from 40.115.56.11\n",
    "To run a command as administrator (user \"root\"), use \"sudo <command>\".\n",
    "See \"man sudo_root\" for details.\n",
    "\n",
    "sshuser@hn0-susie:~$ \n",
    "```\n",
    "then you have successfully connected to your HBase cluster. Now proceed to the next task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with your HBase cluster using the shell\n",
    "\n",
    "In this task we will go through some basic HBase commands, in preparation for the exercise after the next one, where we will import a big dataset and run queries against it.\n",
    "\n",
    "Open the HBase shell by running the following command:\n",
    "\n",
    "**`hbase shell`**\n",
    "\n",
    "Let's say we want to create an HBase table that will store sentences adhering to the structure subject-verb-object (e.g., \"I eat mangoes\", \"She writes books\") in different languages. Here is a schema that we may use:\n",
    "\n",
    "Table name = `sentences`\n",
    "* Column family: `words`\n",
    "  * column: `subject`\n",
    "  * column: `verb`\n",
    "  * column: `object`\n",
    "* Column family: `info`\n",
    "  * column: `language`\n",
    "\n",
    "With the following command we can create such a table (a description of HBase shell commands is available [here](https://learnhbase.wordpress.com/2013/03/02/hbase-shell-commands/)):\n",
    "\n",
    "**`create 'sentences', 'words', 'info'`**\n",
    "\n",
    "You can see the schema of the table with this command:\n",
    "\n",
    "**`describe 'sentences'`**\n",
    "\n",
    "Let's insert some sentences into our table. We will put data cell by cell with the command `put <table>, <rowId>, <columnFamily:columnQualifier>, <value>`:\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:subject', 'I'`**\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:verb', 'drink'`**\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:object', 'coffee'`**\n",
    "\n",
    "Now, let's try to query this sentence from the table:\n",
    "\n",
    "**`get 'sentences', 'row1'`**\n",
    "\n",
    "You should see output similar to this:\n",
    "\n",
    "```\n",
    "COLUMN                                   CELL\n",
    " words:object                            timestamp=1602785046682, value=coffee\n",
    " words:subject                           timestamp=1602785045625, value=I\n",
    " words:verb                              timestamp=1602785045849, value=drink\n",
    "3 row(s) in 0.0910 seconds\n",
    "```\n",
    "\n",
    "As you can see, HBase shell returns data as key-value pairs rather than as rows literally. You may also notice that the lines are lexicographically sorted by the key, which is why \"subject\" appears after \"object\" in the list.\n",
    "\n",
    "I don't know about you, but I like tea more than coffee, so let me update our sentence...\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:object', 'tea'`**\n",
    "\n",
    "As you can see, we are using the same `put` command to *update* a cell. But remember that HBase does not actually update cells in place&mdash;it just inserts new versions instead. If you now run the query again, you will see the new data:\n",
    "\n",
    "**`get 'sentences', 'row1'`**\n",
    "\n",
    "returns:\n",
    "\n",
    "```\n",
    "COLUMN                                   CELL\n",
    " words:object                            timestamp=1602785160890, value=tea\n",
    " words:subject                           timestamp=1602785045625, value=I\n",
    " words:verb                              timestamp=1602785045849, value=drink\n",
    "3 row(s) in 0.0200 seconds\n",
    "```\n",
    "\n",
    "We actually wanted to store sentences in different languages, so let's first set the language for the existing one:\n",
    "\n",
    "**`put 'sentences', 'row1', 'info:language', 'English'`**\n",
    "\n",
    "Note that we are now inserting a value into a different column family but for the same row. Verify with a `get` that this took effect. \n",
    "\n",
    "Now, let's add a sentence in another language (note that we are using another rowID now&mdash;`row2`):\n",
    "\n",
    "**`put 'sentences', 'row2', 'words:subject', 'Ich'`**\n",
    "\n",
    "**`put 'sentences', 'row2', 'words:verb', 'trinke'`**\n",
    "\n",
    "**`put 'sentences', 'row2', 'words:object', 'Wasser'`**\n",
    "\n",
    "**`put 'sentences', 'row2', 'info:language', 'Deutsch'`**\n",
    "\n",
    "Let's check that we indeed have 2 rows now:\n",
    "\n",
    "**`count 'sentences'`**\n",
    "\n",
    "Now, let's query all rows from the table:\n",
    "\n",
    "**`scan 'sentences'`**\n",
    "\n",
    "This, indeed, returns all two rows, in key-value format as before.\n",
    "\n",
    "\n",
    "Of course, you can also scan by column family for column:\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'info'}`**\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'info:language'}`**\n",
    "\n",
    "You can also scan by row ranges (note min incl., max excl.):\n",
    "\n",
    "**`scan 'sentences', {STARTROW=>'row1', ENDROW=>'row3'}`**\n",
    "\n",
    "It is, of course, possible to do some filtering in queries:\n",
    "\n",
    "**`scan 'sentences', {FILTER => \"ValueFilter(=, 'binary:English')\"}`**\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'words:subject', FILTER => \"ValueFilter(=, 'substring:I')\"}`**\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'words:object', ROWPREFIXFILTER => 'row'}`**\n",
    "\n",
    "What if we want to store a sentence that also contains an adjective, in addition to the subject, verb, and object? This is not a problem with HBase, because we can create new columns inside *existing* column families on the fly:\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:subject', 'Grandma'`**\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:verb', 'bakes'`**\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:adjective', 'delicious'`**\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:object', 'cakes'`**\n",
    "\n",
    "This row now has more columns in the `words` column family than others:\n",
    "\n",
    "**`get 'sentences', 'row3'`**\n",
    "\n",
    "We can also add new columns to existing rows:\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:adjective', 'hot'`**\n",
    "\n",
    "**`get 'sentences', 'row1'`**\n",
    "\n",
    "Let's see what happens if you update a value in an existing column:\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:adjective', 'cold'`**\n",
    "\n",
    "You should notice that the time stamp of the column `words:adjective` has been updated.\n",
    "\n",
    "When you are done with the queries, simply type `exit` to quit the hbase shell.\n",
    "\n",
    "Note: to drop a table in HBase, first `disable <table>` then `drop <table>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with your HBase cluster using the Apache Hive\n",
    "\n",
    "It is natural to reuse our prior knowledge from the SQL world. In this section we introduce Apache [Hive](https://hive.apache.org/), the data warehouse software in the Apache family, which facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. A command line tool and JDBC driver are provided to connect users to Hive. We use the command line tool here. For the Python JDBC API, [JayDeBeApi](https://pypi.org/project/JayDeBeApi/) is a good one. \n",
    "\n",
    "Now we want to query the hbase table `sentences` we created in the previous assignment. \n",
    "First, we start Hive with the following command in your ssh connection:\n",
    "\n",
    "**`sshuser@hn0-susie:~$ beeline -u 'jdbc:hive2://localhost:10001/;transportMode=http' -n admin`**\n",
    "\n",
    "You will see now we are in Hive.\n",
    "```\n",
    "Connecting to jdbc:hive2://localhost:10001/;transportMode=http\n",
    "Connected to: Apache Hive (version 1.2.1000.2.6.5.3026-7)\n",
    "Driver: Hive JDBC (version 1.2.1000.2.6.5.3026-7)\n",
    "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
    "Beeline version 1.2.1000.2.6.5.3026-7 by Apache Hive\n",
    "0: jdbc:hive2://localhost:10001/>\n",
    "```\n",
    "Then, we load that table `sentences` from hbase and convert it to a table in Hive. \n",
    "\n",
    "```sql\n",
    "   CREATE EXTERNAL TABLE hbasesent(rowkey STRING, subject STRING, object STRING, \n",
    "   adjective STRING, verb STRING, language STRING)\n",
    "   STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n",
    "   WITH SERDEPROPERTIES ('hbase.columns.mapping' =   \n",
    "   ':key,words:subject,words:object,words:adjective,words:verb,info:language')\n",
    "   TBLPROPERTIES ('hbase.table.name' = 'sentences');\n",
    "```\n",
    "   \n",
    "Now you can use all your SQL knowledge to query the table, e.g., \n",
    "```sql\n",
    "0: jdbc:hive2://localhost:10001/> select subject, verb, object from hbasesent;\n",
    "+----------+---------+---------+--+\n",
    "| subject  |  verb   | object  |\n",
    "+----------+---------+---------+--+\n",
    "| I        | drink   | tea     |\n",
    "| Ich      | trinke  | Wasser  |\n",
    "| Grandma  | bakes   | cakes   |\n",
    "+----------+---------+---------+--+\n",
    "3 rows selected (0.759 seconds)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: if you do not plan to do the next section right now, please delete your cluster and just recreate it when you need it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 &mdash; The Wikipedia dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will see how HBase will handle a large dataset, as well as learn about the filters and caching in HBase.\n",
    "\n",
    "Let's begin. First, SSH to your cluster as in the previous task:\n",
    "\n",
    "**`ssh <ssh_user_name>@<cluster_name>-ssh.azurehdinsight.net`**\n",
    "\n",
    "We provide two datasets for you: [`wiki_small`](https://bigdata2020exassets.blob.core.windows.net/ex05/enwiki-20200920-pages-articles-multistream_small.csv) and [`wiki`](https://bigdata2020exassets.blob.core.windows.net/ex05/enwiki-20200920-pages-articles-multistream.csv). There are stored in a public container you can use throughout the course. But you do not have to download them to your local machine to finish this assignment. \n",
    "The complete dataset comprises approximately the meta data of 20 million articles of the English Wikipedia. You will see the following variables in each csv file: \n",
    "\n",
    "| variable | meaning | Sample value|\n",
    "|:------|:---------------|:---------------|\n",
    "|`page_id`| the page id in the enwiki data dump |1000108|\n",
    "|`page_title`| the page id in the enwiki data dump |AEG Z.6|\n",
    "|`page_ns`| page namespace|0|\n",
    "|`revision_id`| the id of revision to the article |16782282|\n",
    "|`timestamp`| the time a contributor makes a revision |2004-09-19T23:44:33Z|\n",
    "|`contributor_id`| the id of contributor |8817|\n",
    "|`contributor_name`| the name of contributor |Rlandmann|\n",
    "|`bytes`| bytes in revision |21|\n",
    "\n",
    "\n",
    "We will use the `wiki_small` data (about 85MB in csv) in this assignment because it takes shorter time (about 5-10 minutes) to load into HBase. You will be asked to use the bigger dataset (about 2GB in csv) in one of the Moodle assignments and create an HBase table in the same fashion we show you below. Note the bigger set takes 30-40 minutes to load into HBase.\n",
    "\n",
    "Based on the descriptives about the variables above, we can categorize the variables into some about a page and other about an author/contributor.\n",
    "Let us create the schemas in HBase now with two column families `page` and `author`\n",
    "\n",
    "**`hbase shell`**\n",
    "\n",
    "**`create 'wiki_small', 'page', 'author'`**\n",
    "\n",
    "After the table is created, we need to exit the HBase shell and return back to the head node's shell:\n",
    "\n",
    "**`exit`**\n",
    "\n",
    "Now we need to populate both tables with data. We will use the [ImportTsv](https://hbase.apache.org/book.html#importtsv) utility of HBase. \n",
    "\n",
    "Populate the table `wiki_small` by running the following command. We need to specify which column in the csv maps to which column in the HBase table. Note that we make `page_id` into the `HBASE_ROW_KEY`. Note that these commands print a lot of messages, but they are mostly informational with an occasional non-critical warning; unless something goes wrong, of course :) The commands will also report some \"Bad Lines\", but you can safely ignore this&mdash;some lines may contain illegal characters and be dropped, but most of the data is in good shape.\n",
    "\n",
    "**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY,page:page_title,page:page_ns,page:revision_id,author:timestamp,author:contributor_id,author:contributor_name,page:bytes\" wiki_small wasbs://ex05@bigdata2020exassets.blob.core.windows.net/enwiki-20200920-pages-articles-multistream_small.csv`**\n",
    "\n",
    "Note here we use the csv we have uploaded to a public container `ex05` and you can directly populate your HBase table with the data from that container. And you see how we specify the mappings between the csv columns and the column family:column in the HBase table.\n",
    "\n",
    "You can count how many rows there are using this command from your head node's shell:\n",
    "**`hbase org.apache.hadoop.hbase.mapreduce.RowCounter 'wiki_small'`**\n",
    "\n",
    "Similarly, after you create an HBase table `wiki`, you can also populate the `wiki` table using the following command for the Moodle assignment.\n",
    "\n",
    "**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY, page:page_title,page:page_ns,page:revision_id,author:timestamp,author:contributor_id,author:contributor_name,page:bytes\" wiki wasbs://ex05@bigdata2020exassets.blob.core.windows.net/enwiki-20200920-pages-articles-multistream.csv`**\n",
    "\n",
    "Now let's go into HBase shell again and run some queries against the `wiki_small` table. We will look at some of the filters listed by HBase if you run `show_filters` in an HBase shell, e.g., `PrefixFilter(), ValueFilter(), SingleColumnValueFilter()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks to do with the table `wiki_small`\n",
    "\n",
    "1. How does HBase index the row keys? \n",
    "We choose `page_id` in the original table to be the row keys in the HBase table. Run these two queries and what do you observe? What can we say about row key indexing based their results? \n",
    "\n",
    "    **`scan 'wiki_small', {STARTROW=>'100009', ENDROW=>'100011'}`**\n",
    "\n",
    "    **`scan 'wiki_small', {STARTROW=>'100015', ENDROW=>'100016'}`**\n",
    "\n",
    "1. Write the following queries:\n",
    "  1. Select all article titles and author names where the row name starts with '`1977`'\n",
    "  1. Select all article titles and author names where the author contains the substring '`tom`'. \n",
    "  \n",
    "1. Write the following queries:\n",
    "  1. Return the number of articles from 2017.\n",
    "  1. Return the number of articles that contain the word `Attacks` on them. Please discuss different possibilities to formulate this query. \n",
    "  \n",
    "1. Execute your queries more than once and observe the query execution times\n",
    "1. What are the advantages and disadvantages of pure row stores?\n",
    "1. What are the advantages and disadvantages of pure column stores?\n",
    "1. What are the advantages and disadvantages of wide column stores?\n",
    "1. What are the advantages and disadvantages of denormalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "1. **`scan 'wiki_small', {STARTROW=>'100009', ENDROW=>'100011'}`** returns five keys `1000092, 1000102, 1000104, 1000106, 1000108`, **`scan 'wiki_small', {STARTROW=>'100015', ENDROW=>'100016'}`** returns seven keys `100015, 1000151, 1000153, 1000155, 1000156, 1000158, 1000159`. Rows are sorted lexicographically by row key. You may have to pad keys to get the sorting order you really want (without padding 10 is lexicographically smaller than 2). \n",
    "\n",
    "1. The two queries:\n",
    "  1. All article titles and author names where the row name starts with '1977':\n",
    "    1. Two ways of formulating the queries:\n",
    "    \n",
    "       `scan 'wiki_small', {COLUMNS => 'page:page_title', ROWPREFIXFILTER => '1977'}`\n",
    "       \n",
    "       or\n",
    "       \n",
    "       `scan 'wiki_small', {COLUMNS => 'page:page_title', FILTER => \"PrefixFilter('1977')\"}`\n",
    "    \n",
    "    1. `scan 'wiki_small', {COLUMNS => 'author:contributor_name', ROWPREFIXFILTER => '1977'}`\n",
    "    \n",
    "  1. All article titles and author names where the author name contains the substring '`tom`'\n",
    "  \n",
    "    1. `scan 'wiki_small', {COLUMNS => 'page:page_title', FILTER => \"ValueFilter(=, 'substring:tom')\"}`\n",
    "    \n",
    "    1. `scan 'wiki_small', {COLUMNS => 'author:contributor_name', FILTER => \"ValueFilter(=, 'substring:tom')\"}`\n",
    "    \n",
    "1. The queries counting the rows.\n",
    "  1. Number of articles from 2017.\n",
    "      First, see if we manage to get the articles from 2017 using the following query, limit the return rows to five.\n",
    "      \n",
    "      `scan 'wiki_small', {COLUMNS =>'author:timestamp', FILTER => \"ValueFilter(=, 'substring:2017')\", LIMIT=>5}`\n",
    "  \n",
    "      Then run \n",
    "      \n",
    "      `scan 'wiki_small', {COLUMNS =>'author:timestamp', FILTER => \"ValueFilter(=, 'substring:2017')\"}`\n",
    "      \n",
    "      , which gives the returned row count 83647. \n",
    "      \n",
    "   1. Return the number of articles that contain the word `Attacks` on them.\n",
    "   \n",
    "      Run this query \n",
    "      \n",
    "      `scan 'wiki_small', {COLUMNS =>'page:page_title', FILTER => \"ValueFilter(=, 'substring:Attacks')\"}`\n",
    "      \n",
    "      which returns 96 rows.\n",
    "      \n",
    "      Another way of formulating the query is \n",
    "      \n",
    "      `scan 'wiki_small', {FILTER => \" SingleColumnValueFilter ('page', 'page_title', =, 'substring:Attacks') \"}` \n",
    "\n",
    "1. Execution times\n",
    "  1. Queries with `ROWPREFIXFILTER` should be quick for both tables, because the filter is applied to the row key rather than to the contents of columns. \n",
    "  1. Subsequent invocations of the same command take less time due to caching.\n",
    "1. **Pure row store:**\n",
    "  1. Advantages:\n",
    "    1. Good for workloads with point lookups and updates. A point lookup query returns only one or a small number of distinct rows. Retrieving (updating) a single row is efficient as the row is colocated\n",
    "  1. Disadvantages:\n",
    "    1. Scans are more expensive (whole row is always retrieved)\n",
    "1. **Pure column store:**\n",
    "  1. Advantages:\n",
    "    1. Scans are very efficient (only specific columns can be retrieved)\n",
    "  1. Disadvantages:\n",
    "    1. To retrieve (or update) a whole row, many random accesses need to be performed\n",
    "1. **Wide column store:**\n",
    "  1. Advantages:\n",
    "    1. Column families offer a 'middle ground' between pure row- and column-oriented storages.  Columns frequently accessed together can be colocated, very wide columns (affecting scan speed) can be isolated into separate column families\n",
    "    1. Flexible schema (column names stored for each row) offer flexibility for cases where schema is not known upfront (or in cases of sparse columns)\n",
    "  1. Disadvantages\n",
    "    1. Performance penalties, point lookups not as fast as pure row store, scans not as fast as pure column store\n",
    "    1. Storage overhead\n",
    "1. **Denormalization:**\n",
    "  1. Advantages:\n",
    "    1. All operations are either scans or point lookups. No need for expensive joining of multiple relations (all data is colocated or easily mapped)\n",
    "  1. Disadvantages:\n",
    "    1. It is difficult to enforce (maintain) consistency in cases of updates\n",
    "    1. Storage (memory) overhead, due to duplicated data\n",
    "    1. Scan processing can be more expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: delete your HBase cluster now. As has been said above, these clusters burn Azure credit very fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 &mdash; Architecture of HBase\n",
    "\n",
    "In the previous tasks, we have seen HBase in action. Let us now take a look at the internal architecture of HBase. You may want to consult the lecture slides when solving these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1 &mdash; Inside a RegionServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will see how a RegionServer in HBase would execute a query.\n",
    "\n",
    "Imagine that we have an HBase table called '`phrases`', which has the following schema:\n",
    "\n",
    "* Column family: `words`\n",
    "  * column: A\n",
    "  * column: B\n",
    "  * column: C\n",
    "  * (potentially also columns D, E, F, etc.)\n",
    "\n",
    "Thus, the table has only one column family. Each column in this family holds one word.\n",
    "\n",
    "Recall from the lecture slides that keys in HBase have the following structure:\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/hbase-key-structure.png\" width=\"70%\">\n",
    "\n",
    "We need make certain simplifications to the format of keys to avoid excessive clutter in this exercise. Since the table in this exercise has only one column family, we will omit it from the key and will only specify the column name (A,B,C, ...). We will also omit the length fields and the \"key type\" field. The timestamp field in this exercise will contain integers from 1 to 10, where in reality it would contain the number of milliseconds since an event in the long past. Thus, keys as will be used in this exercise consist of three fileds: row, column, timestamp.\n",
    "\n",
    "### Tasks to do\n",
    "\n",
    "State which Key-Value pairs will be returned by each of the following queries, given in HBase shell syntax which you have already seen in the first exercise. Assume that the HBase instance is configured to return only the latest version of a cell.\n",
    "\n",
    "1. `get 'phrases', '278'`\n",
    "1. `get 'phrases', '636'`\n",
    "1. `get 'phrases', '593'`\n",
    "1. `get 'phrases', '640'`\n",
    "1. `get 'phrases', '443'`\n",
    "\n",
    "To answer this question, use the diagram below, which represents the state of a RegionServer responsible for the row region in the range of row IDs 100&ndash;999, which is the region into which all these queries happen to fall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A larger, zoomable, PDF version of this diagram is available [here](https://bigdata2020exassets.blob.core.windows.net/ex05/BD_Ex05_Overall_Instance1.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overall_Instance1](https://bigdata2020exassets.blob.core.windows.net/ex05/BD_Ex05_Overall_Instance1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to the Task 3.1\n",
    "\n",
    "1. get 'phrases', '278' \n",
    "\n",
    "| Row | Column | Timestamp | Value | Where it came from |\n",
    "|:-----:|:-----:|:-----:|:-------|:--------------------:|\n",
    "|278|A|8|cake|HFile1|\n",
    "|278|B|1|is|HFile2|\n",
    "|278|C|5|a lie|HFile3|\n",
    "\n",
    "2. get 'phrases, '636'\n",
    "\n",
    "| Row | Column | Timestamp | Value | Where it came from |\n",
    "|:-----:|:-----:|:-----:|:-------|:--------------------:|\n",
    "|636|A|4|watch|HFile2|\n",
    "|636|B|1|your|MemStore|\n",
    "|636|C|1|step|MemStore|\n",
    "\n",
    "3. get 'phrases', '593'\n",
    "\n",
    "| Row | Column | Timestamp | Value | Where it came from |\n",
    "|:-----:|:-----:|:-----:|:-------|:--------------------:|\n",
    "|593|A|1|indeed|HFile2|\n",
    "\n",
    "4. get 'phrases', '640'\n",
    "\n",
    "| Row | Column | Timestamp | Value | Where it came from |\n",
    "|:-----:|:-----:|:-----:|:-------|:--------------------:|\n",
    "|640|A|5|long|HFile1|\n",
    "|640|B|6|live|HFile2|\n",
    "|640|C|2|rock'n'roll|MemStore|\n",
    "\n",
    "Note that \"640,C,1 -> the king\" (HFile1) got superseded by \"640,C,2 -> rock'n'roll\".\n",
    "\n",
    "5. get 'phrases', '443'\n",
    "\n",
    "The row doesn't exist, so no output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2 &mdash; Bloom filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with summarizing what we have in memory when working with HBase: MemStore, LRU (Least Recently Used) BlockCache, Indices of HFiles, and Bloom Filters. We did not have time to cover it in the lecture, but bloom filters are actually very crucial in avoiding disk reads if we can guarantee that a key is **not** in an HFile. Bloom filters allow us to very quickly determine whether an element belongs to a set.\n",
    "\n",
    "Bloom filters are a data structure used to speed up queries, useful in the case in which it's likely that the value we are looking doesn't exist in the collection we are querying. Their main component is a bit array with all values initially set to 0. When a new element is inserted in the collection, its value is first run through a certain number of (fixed) hash functions, and the locations in the bit array corresponding to the outputs of these functions are set to 1.\n",
    "\n",
    "This means that when we query for a certain value, if the value has previously been inserted in the collection then all the locations corresponding to the hash function outputs will certainly already have been set to 1. On the contrary, if the element hasn't been previously inserted, then the locations may or may not have already been set to 1 by other elements.\n",
    "\n",
    "Then, if prior to accessing the collection we run our queried value through the hash functions, check the locations corresponding to the outputs, and find any of them to be 0, we are guaranteed that the element is not present in the collection (No False Negatives), and we don't have to waste time looking. If the corresponding locations are all set to 1, the element may or may not be present in the collection (Possibility of False Positives), but in the worst case we're just wasting time.\n",
    "\n",
    "Inspect the following examples. Say we have hash functions that map the input `John Smith` and `Mary Smith` to the bit array `011001110000`. When we have a new input `Albert Einstein` which is mapped by the same hash functions to the bit array `011001110000`. This clearly does not correspond to the bit array produced by the previous two inputs. Hence, we can say that `Albert Einstein` is not in the set which `John Smith` and `Mary Smith` belong to (denoted as `{the Smiths}` for short). However, another input `Louis de Broglie` whose bit array after hashing is `011001110000` is then a false positive for the set `{the Smiths}`. \n",
    "\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/bloomfilter1.png\" width=\"50%\">\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/bloomfilter2.png\" width=\"50%\">\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/bloomfilter3.png\" width=\"50%\">\n",
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/bloomfilter4.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen in the task above, HBase has to check all HFiles, along with the MemStore, when looking for a particular key. As an optimisation, Bloom filters are used to avoid checking an HFile if possible. Before looking inside a particular HFile, HBase first checks the requested key against the Bloom filter associated with that HFile. If it says that the key does not exist, the file is not read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will look at how Bloom filters work. We will use a list of words to populate a Bloom filter and we will then query it.\n",
    "\n",
    "Bloom filter requires several hash functions. To keep things easily computable by a human, we will define the following three hash functions for the purpose of this exercise:\n",
    "\n",
    "1. Given an English word $x$, the value of the first hash function, $hash_1(x)$, is equal to the *first letter of the word*. E.g.: $hash_1($\"`federal`\"$)$ = \"`f`\"\n",
    "\n",
    "1. Given an English word $x$, the value of the second hash function, $hash_2(x)$, is equal to the *second letter of the word*. E.g.: $hash_2($\"`federal`\"$)$ = \"`e`\"\n",
    "\n",
    "1. Given an English word $x$, the value of the third hash function, $hash_3(x)$, is equal to the *third letter of the word*. E.g.: $hash_3($\"`federal`\"$)$ = \"`d`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bloom filter starts with a bit array which has value \"0\" recorded for each possible output value of all three hash functions (or, for example, modulo the size of the bit array, if the output range of the hash functions is too large).\n",
    "When we *add* an element to a Bloom filter, we compute the three values of the three hash functions and set those locations in the Bloom filter to \"1\". For example, if we add \"`federal`\" to the Bloom filter using the three hash functions that we have defined above, we get the following:\n",
    "\n",
    "| | | |1|1|1| | | | | | | | | | | | | | | | | | | | |\n",
    "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "|A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|Y|Z|\n",
    "\n",
    "Here, only values \"1\" are displayed to avoid cluttering the view; thus, if a cell is empty, it is assumed to hold a \"0\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, populate the following table (double-click the table to edit it and hit Ctrl+Enter to exit the editing mode; you are also free to do this task in some other tool, of course):\n",
    "\n",
    "| Word    | $hash_1$ | $hash_2$ | $hash_3$ |\n",
    "|:--------|-------------|-------------|-------------|\n",
    "|round    |             |             |             |\n",
    "|sword    |             |             |             |\n",
    "|past     |             |             |             |\n",
    "|pale     |             |             |             |\n",
    "|nothing  |             |             |             |\n",
    "|darkness |             |             |             |\n",
    "|water    |             |             |             |\n",
    "|feet     |             |             |             |\n",
    "|thin     |             |             |             |\n",
    "|passage  |             |             |             |\n",
    "|corner   |             |             |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, *add* each word from the list into the following Bloom filter (you can also double-click to edit it; you can double-click the Bloom filter populated with \"federal\" above to see an example of a filled-in filter):\n",
    "\n",
    "| | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
    "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "|A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|Y|Z|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word from the following list, state whether this Bloom filter reports it as belonging to the set or not (skip filling-in the hash columns, if you want):\n",
    "\n",
    "| Word    | $hash_1$ | $hash_2$ | $hash_3$ | The Bloom filter says the word belongs to the set: (yes/no) |\n",
    "|:--------|----------|----------|----------|:-----------:|\n",
    "|sword    |          |          |          |             |\n",
    "|sound    |          |          |          |             |\n",
    "|psychic  |          |          |          |             |\n",
    "|pale     |          |          |          |             |\n",
    "|book     |          |          |          |             |\n",
    "|deaf     |          |          |          |             |\n",
    "|truss    |          |          |          |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the words that were flagged by the Bloom filter as belonging to the set are actually **not** in the set (a *false positive* outcome)?\n",
    "\n",
    "Which of the words that were flagged by the Bloom filter as **not** belonging to the set actually **do belong** to the set (a *false negative* outcome)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to the Task 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashes for each of the words from the first list:\n",
    "\n",
    "| Word    | $hash_1$ | $hash_2$ | $hash_3$ |\n",
    "|:--------|-------------|-------------|-------------|\n",
    "|round    |R            |O            |U            |\n",
    "|sword    |S            |W            |O            |\n",
    "|past     |P            |A            |S            |\n",
    "|pale     |P            |A            |L            |\n",
    "|nothing  |N            |O            |T            |\n",
    "|darkness |D            |A            |R            |\n",
    "|water    |W            |A            |T            |\n",
    "|feet     |F            |E            |E            |\n",
    "|thin     |T            |H            |I            |\n",
    "|passage  |P            |A            |S            |\n",
    "|corner   |C            |O            |R            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bloom filter with all those words added to it:\n",
    "\n",
    "|1| |1|1|1|1| |1|1| | |1| |1|1|1| |1|1|1|1| |1| | | |\n",
    "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "|A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|Y|Z|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word from the following list, state whether this Bloom filter marks them as belonging to the set or not:\n",
    "\n",
    "| Word    | The Bloom filter says the word belongs to the set: (yes/no) | Comment |\n",
    "|:--------|:-----------:|:-------------|\n",
    "|sword    | yes            | present in the original list |\n",
    "|sound    | yes            | not from the original list, but still got flagged as present: this is a *false positive* |\n",
    "|psychic  | no             | indeed, it's not from the original list |\n",
    "|pale     | yes            | present in the original list |\n",
    "|book     | no             | indeed, it's not from the original list |\n",
    "|deaf     | yes            | a *false positive* |\n",
    "|truss    | yes            | a *false positive* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a Bloom filter can produce false positive outcomes. Luckily, it never produces false negative outcomes, i.e., if a Bloom filter says that an element is absent from the set, it is guaranteed to really be absent from the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3 &mdash; Building an HFile index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing a get, the RegionServer needs to check its MemStore and all HFiles (unless the Bloom filter returns negative) for the existence of the requested key. In order to avoid scanning HFiles entirely, HBase uses index structures to quickly skip to the position of the *HBase block* which may hold the requested key. Note HBase block is not to be confused with HDFS block and the underlying file system block, see [here](https://blog.cloudera.com/hbase-blockcache-101/#3) for a good discussion. HBase blocks come in 4 varieties: DATA, META, INDEX, and BLOOM.\n",
    "\n",
    "By default, each *HBase block* is 64KB (configurable) in size and always contains whole key-value pairs, so, if a block needs more than 64KB to avoid splitting a key-value pair, it will just grow. \n",
    "\n",
    "In this task, you will be building the index of an HFile. __For the purpose of this exercise__, assume that each HBase block is 40 bytes long, and each character in keys and values is worth 1 byte: for example, the first key-value pair in the diagram below is worth $3 + 1 + 1 + 6 = 11$ bytes. Below this diagram you will find a table for you to fill in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/BD_Ex05_LSM_Instance1.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the contents of the HFile above, you need to populate the index, following the approach described in the lecture slides. Use the following table (again, you can edit it by double-clicking). Use as many or as few rows as you need.\n",
    "\n",
    "| RowId | Column | Version |\n",
    "|-------|--------|---------|\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to the Task 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://bigdata2020exassets.blob.core.windows.net/ex05/BD_Ex05_LSM_Instance1_Solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 &mdash; Thinking about the schema (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very important schema design question in HBase is the choice of the row key.\n",
    "\n",
    "Imagine that you have a dataset containing:\n",
    "* addresses of websites (URLs), potentially of all websites available online\n",
    "* for each URL: the country in which the owner of the website is registered\n",
    "* for each URL and for each country in the world: the number of visits to that URL from that country during the last month\n",
    "\n",
    "You plan to store this dataset in HBase. For each of the following queries, state what you think is the best choice for the row key:\n",
    "1. Given a particular URL, count the total number of visits\n",
    "1. Given a particular country, find the URL that is visited the most by the users from that country\n",
    "1. Among all URLs whose owners are registered in a particular country, find the most visited one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Exercise 4\n",
    "\n",
    "1. **Given a particular URL, count the total number of visits.** In this case we can just have the URL as the row key, because we look-up by URL alone.\n",
    "1. **Given a particular country, find the URL that is visited the most by the users from that country.** Here it is better to have a key consisting of the visitor country and URL concatenated together. This way we will be able to run fast queries with a `ROWPREFIXFILTER`. Note that in this case the owner's country will have to be duplicated for each row that has the same URL in the key, so the data is denormalized.\n",
    "1. **Among all URLs whose owners are registered in a particular country, find the most visited one.** In this case it makes sense to have the owner's country prepended to the URL for similar reasons as in the previous case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 &mdash; Log-structured mergetree (LSM tree) (optional) \n",
    "\n",
    "We learn in the lecture that LSM tree is highly efficient in applications using wide column storage such as HBase, Cassandra, BigTable, LevelDB, where insertions in memory happen quite often. As opposed to B+-tree which has a time complexity of *O(log n)* when inserting new elements, *n* being the total number of elements in the tree, LSM tree has *O(1)* for inserting, which is a constant cost. In the reading of this week, you can find [more](https://proquest.safaribooksonline.com/book/databases/database-design/9781449314682/seek-versus-transfer/archbplustrees_html#X2ludGVybmFsX0h0bWxWaWV3P3htbGlkPTk3ODE0NDkzMTQ2ODIlMkZhcmNobHNtdHJlZXNfaHRtbCZxdWVyeT1sc20=) on this topic.\n",
    "\n",
    "The following figure is from the HBase Guide book where we see how a multipage block is merged from the in-memory tree into the next on-disk tree. Trees in the store files are arranged similar to B-trees. Merging writes out a new block with the combined result. Eventually, the trees are merged into the larger blocks.<br>\n",
    "![](https://bigdata2020exassets.blob.core.windows.net/ex05/lsm.png)\n",
    "\n",
    "Inserting data into LSM Tree:\n",
    "\n",
    "1. When a write comes, it is inserted in the memory-resident MemStore.\n",
    "2. When the size of the MemStore exceeds a certain threshold, its flushed to the disk.\n",
    "3. As MemStore is already sorted, creating a new HFile segment from it is efficient enough.\n",
    "4. Old HFiles are periodically compacted together to save disk space and reduce fragmentation of data.\n",
    "\n",
    "Reading data from LSM Tree:\n",
    "\n",
    "1. A given key is first looked up in the MemStore.\n",
    "1. Then using a hash index its searched in one or more HFiles depending upon the status of the compaction.\n",
    "\n",
    "Deletes are a special case of update wherein a delete marker is stored and is used during the lookup to skip deleted keys. When the pages are rewritten asynchronously, the delete markers and the key they mask are eventually dropped.\n",
    "\n",
    "We will now walk through a concrete exercise to understand how LSM tree works in HBase. Image we have a client who is constantly writing into HBase. The client also occasionally reads and deletes key value pairs in HBase. MemStore and disk are two storages we examine in this assignment. \n",
    "\n",
    "<img align=\"center\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex01.png\"><br>\n",
    "The client requests the following operations from HBase in sequence:\n",
    "1. writing the following key value pairs into HBase: `(C,1), (B,2), (A,9), (A,109), (G,8), (D,67), (Z,0)`;\n",
    "2. reading the value of key `A`;\n",
    "3. deleting the key `Z`;\n",
    "4. writing the following key value pairs: `(S,100), (Z1,900), (A1,9), (A01,1), (A11,1)`. \n",
    "\n",
    "In the meanwhile, in HBase flush and compaction are conducted to optimize transfer. \n",
    "\n",
    "Please draw the processes of \n",
    "1. how the key value pairs are stored in HBase? To simplify the actual key in HBase, we use key[t] to denote the key. E.g., when the client writes the key value pair `(C,1)` into HBase, it is first stored in MemStore with the key value `C[t1] 1`.\n",
    "<img align=\"center\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex02.png\">\n",
    "2. how does HBase flush and compact the HFiles? Let us set the threshold of flush to three, i.e., when the key value pairs in MemStore have reached three, HBase will flush them to disk (from $C_0$ to $C_1$). Let us also set the threshold of compaction to three, i.e., if there exist two HFiles, each with three key value pairs, we have to compact them into a bigger HFile (from $C_1$ to $C_2$). The same rule applies for bigger HFiles: whenever there is of factor two some HFile at the level of $C_{k-1}$, compact them to the level $C_k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Exercise 5\n",
    "Here is a chain of operations that would happen. Please read the charts from left to right in a row and then go to the next row.\n",
    "\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex01.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex02.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex03.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex04.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex05.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex06.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex07.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex08.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex09.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex10.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex11.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex12.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex13.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex14.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex15.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex16.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex17.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex19.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex20.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex21.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex22.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex23.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex24.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex25.png\">\n",
    "<img align=\"left\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex26.png\">\n",
    "<img align=\"right\" width=\"50%\" src=\"https://bigdata2020exassets.blob.core.windows.net/ex05/LSM-ex27.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
