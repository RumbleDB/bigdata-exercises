{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Big Data – Exercises</center>\n",
    "## <center>Fall 2025 – Week 8 – ETH Zurich</center>\n",
    "## <center>Notebook for the Spark RDD Moodle Quiz</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start docker\n",
    "\n",
    "1. Drag this notebook (and everything else in the exercise08 folder) in the `notebooks` folder of your exam magic box\n",
    "\n",
    "2. Start docker with ```docker-compose up -d```\n",
    "\n",
    "3. Launch Jupiter from the docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Moodle Quiz\n",
    "\n",
    "For this quiz we will be using the [language confusion dataset](https://lars.yencken.org/datasets/great-language-game).\n",
    "\n",
    "\n",
    "For the Moodle quiz we ask you to submit the following things:\n",
    "- The query you wrote (although it is not graded, having your query is helpful for arguing about points)\n",
    "- Something related to its output (**the only part that is graded**)\n",
    "\n",
    "You don't need to submit this notebook, only the queries you wrote.\n",
    "\n",
    "You will need the same language game dataset used in the exercises. If you still do not have created the `confusion-part.json` file, follow these instruction.\n",
    "\n",
    "1. Move to the `notebooks` folder in the terminal\n",
    "2. Download the data: <br>\n",
    "   ```wget https://f003.backblazeb2.com/file/larsyencken-eu-public/greatlanguagegame/confusion-2014-03-02.tbz2``` <br>\n",
    "   __or__ <br>\n",
    "   ```curl -O https://f003.backblazeb2.com/file/larsyencken-eu-public/greatlanguagegame/confusion-2014-03-02.tbz2```\n",
    "3. Extract the data: <br>\n",
    "   ```tar -jxvf confusion-2014-03-02.tbz2```\n",
    "4. Change directory to ```confusion-2014-03-02```\n",
    "5. Extract the part of the dataset that we will work with in this exercise: <br>\n",
    "   ```head -n 3000000 confusion-2014-03-02.json > confusion-part.json```\n",
    "\n",
    "Alternatively, you can download the preprocessed dataset from our [Polybox](https://polybox.ethz.ch/index.php/s/zgZY2dCPAXn6HA3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing commands\n",
    "In your newly created notebook run these commands in order to have the dataset into an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 350.889892578125,
      "end_time": 1604594457525.873
     }
    }
   },
   "outputs": [],
   "source": [
    "# watch out that your confusion-part.json is in the same folder as this notebook!\n",
    "path = \"confusion-part.json\"\n",
    "raw_data = sc.textFile(path)\n",
    "dataset = raw_data.map(json.loads).cache()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the entries are JSON records, you will need to parse them and use their respective object representations. You can use this mapping for all queries.\n",
    "\n",
    "**For the quiz, fill in the results by running the queries on the ```confusion-part.json``` subset instead of the entire dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that you will be able to run the queries of the moodle question of this week. The RDD that you have to perform your queries on is the ```dataset``` one. For example, the following command returns one element of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 36261.0908203125,
      "end_time": 1604594507380.752
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Let's get to work. A few last things:\n",
    "- Take into account that some of the queries might have very large outputs, which Jupyter (or sometimes even Spark) won't be able to handle. It is normal for the queries to take some time, but if the notebook crashes or stops responding, try restarting the kernel. Avoid printing large outputs. You can print the first few entries to confirm the query has worked, as shown in the query above.\n",
    "- Refer to the [documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html) whenever in doubt.\n",
    "\n",
    "And now to the actual queries: *Please make sure that in your queries you *only* use PySpark RDDs, and avoid any dataframes (they will covered in next week's exercises)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "Count the games where the guess is incorrect but the correct language (target) appears within the first two choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "Return the five most common confusion pairs (target, guess) where the guess was wrong, ordered by frequency (desc).\n",
    "\n",
    "**Hint**: You may want to have a look at the documentation for [sortByKey](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortByKey.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "Find the three countries with the highest accuracy (percentage of correct guesses), considering only countries with at least 100 games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "Return the date with the most games played and the corresponding count (dates like YYYY-MM-DD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5\n",
    "Return the number of distinct sample IDs for games played in 2013 (dates like YYYY-MM-DD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 6\n",
    "Among languages with accuracy $\\ge$ $0.9$, return the one with the largest number of games (along with its total count and accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
