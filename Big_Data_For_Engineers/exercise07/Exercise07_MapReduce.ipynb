{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Big Data for Engineers – Exercises</center>\n",
    "## <center>Spring 2022 – Week 7 – ETH Zurich</center>\n",
    "## <center>Map Reduce </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we will review _MapReduce_—a programming model and distributed system for processing datasets in parallel over large clusters. We will first discuss the two APIs that can be used to write MapReduce jobs. Then, we willimplement a MapReduce job in Python. Finally, we will discuss relevant theory bits behind MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. The two APIs for MapReduce\n",
    "\n",
    "MapReduce provides two different interfaces: the **native MapReduce API** and the **Streaming API**.\n",
    "\n",
    "The native MapReduce API is equivalent to the one seen in class. To use it, the user has to write two Java classes: one for the Mapper function and one for the Reducer function. Just like in the logical model:\n",
    "\n",
    "- the Mapper function takes a KeyValue pair and emits zero or more KeyValue pairs;\n",
    "```js\n",
    "function map(key, value)\n",
    "  // Do some work\n",
    "  emit(someKey, someValue)\n",
    "```\n",
    "- and the Reducer function takes a key and a collection of values, and emits zero or more KeyValue pairs (usually one). \n",
    "```js\n",
    "function reduce(key, values[])\n",
    "  // Do some work\n",
    "  emit(key, aggregatedValue)\n",
    "```\n",
    "  \n",
    "The Streaming API is usually slower, but allows users to write the Mapper and the Reducer functions in any language — even different languages.\n",
    "To use the API we need to write two functions, a mapper and a reducer. In this case, however, the programs will directly read the KV pairs as a sequence of lines from standard input and write the resulting KV pairs to standard output.\n",
    "The streaming API will take care of all the parallelization, the shuffling and everything else required.\n",
    "Since the operations are done using the standard input and standard output, there are two differences with the logical model:\n",
    "\n",
    "1. The KV pairs are not independently processed, but read all **sequentially** from standard input.\n",
    "2. The reducer task does not receive a key with a list of values, but the **ordered list** of KV pairs (one per line). Therefore, the reducer must therefore implement itself the logic for handling a new key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hands on\n",
    "\n",
    "In this first part of the exercise session, we will obtain some practical experience with MapReduce. To do so, we will simulate the cluster enviornment by docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Start the Hadoop Enviornment\n",
    "\n",
    "Navigate to the exercise07 folder, bring up docker. It will start several containers for you to simulate the cluster enviornment.\n",
    "\n",
    "``` bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "Wait until docker finish downloading all the images. This may take several minutes. You should be able to open jupyter notebook using the URL of `localhost:8888` in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Writing the mapper and the reducer functions\n",
    "\n",
    "To run a MapReduce job we need of course a mapper function and a reducer. Usually to run natively on Hadoop we need to write our mapper and reducer as classes in Java, but to make the development easier and less cumbersome we are going to use the **Hadoop streaming API**.  \n",
    "This wonderful API allows the developers to write code in any language and integrate it seamlessly with the MapReduce framework.  \n",
    "We just need to provide 2 scripts—one for the mapper, one for the reducer—and let them read the KeyValues from `stdin` (the default input stream) and write them to `stdout` (the default output stream). Hadoop will take care of parallelization, the sort step and everything else required.  \n",
    "\n",
    "To start we will just use the HelloWorld for MapReduce programs, which is WordCount: given a list of files, return for each word the total number of occurrences.  \n",
    "\n",
    "Copy our sample file to the NameNode container's root directory (make sure you are at the exercise07 directory):\n",
    "\n",
    "``` bash\n",
    "docker cp mapper.py namenode:/\n",
    "docker cp reducer.py namenode:/\n",
    "```\n",
    "\n",
    "Loggin to the NameNode. \n",
    "\n",
    "``` bash\n",
    "docker exec -it namenode /bin/bash\n",
    "```\n",
    "\n",
    "List the directory to make sure that the python files are correctly copied to the NameNode:\n",
    "\n",
    "`ls *py`\n",
    "\n",
    "You can also bring up the terminal by using Docker's graphical user interface. But we recommend that you gradually get used to the command line, because for many tools there is no option of using a GUI. \n",
    "\n",
    "Before continuing we need to ensure that the files are actually executable so they can be run by the MapReduce job  \n",
    "`chmod +x ./reducer.py ./mapper.py`.\n",
    "\n",
    "Let's take a closer look at the files:\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"mapper.py\"\"\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Set to lowercase, remove punctuations and tokenize\n",
    "    line = line.lower().strip()\n",
    "    line = re.sub(r\"[^\\w\\s]\", \"\", line)\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        print('%s\\t%s' % (word, 1))\n",
    "```\n",
    "For the mapper the first line starting with `#!` tells to Hadoop how to run the script (using Python in this case), then for each line in our input (automatically directed to the `sys.stdin` stream by Hadoop) we remove leading and trailing whitespaces, then split the line on each whitespace and 'emit' a key-value pair made up of a word and the unit count one.\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"reducer.py\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print('%s\\t%s' % (current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "if current_word == word:\n",
    "    print('%s\\t%s' % (current_word, current_count))\n",
    "\n",
    "```\n",
    "For the reducer we receive an ordered list of key-value pairs generated by the mapper and then sorted automatically, so for each line in the input stream, we remove leading and trailing whitespaces, we split the line into the word and the count (always 1 if we used the previous mapper and no combiner), then try to convert the count (by default a string) to a number (and skipping the value in case of failure).  \n",
    "After that if the word is equal to the previous one (remember the kv pairs are sorted so all the same words will be together) we just increase the count for that word by one, otherwise we print the current word with the associated count and move to the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Test correctness of your program locally\n",
    "\n",
    "Since a MapReduce job on a cluster usually requires a considerable amount of time, before launching it we want to test our functions locally.  \n",
    "To do so we can simulate our MapReduce job by inputting the data to the mapper, properly sorting the output of that and feeding it into the reducer, then checking that we get the expected result.  \n",
    "\n",
    "We can try with   `echo \"foo foo quux labs foo bar quux\" | ./mapper.py | sort -k1,1 | ./reducer.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Get some data\n",
    "\n",
    "Download on the cluster some nice books on which we will run our MapReduce job (btw some of these are really nice)\n",
    "```\n",
    "wget http://www.gutenberg.org/cache/epub/2500/pg2500.txt \n",
    "wget http://www.gutenberg.org/files/1342/1342-0.txt\n",
    "wget http://www.gutenberg.org/files/84/84-0.txt\n",
    "wget http://www.gutenberg.org/files/2600/2600-0.txt\n",
    "wget http://www.gutenberg.org/files/74/74-0.txt\n",
    "wget http://www.gutenberg.org/files/2591/2591-0.txt\n",
    "wget http://www.gutenberg.org/files/4300/4300-0.txt\n",
    "```\n",
    "\n",
    "and put them on HDFS\n",
    "```\n",
    "hadoop fs -mkdir /books\n",
    "hadoop fs -copyFromLocal ./*.txt /books\n",
    "```\n",
    "\n",
    "There will be some encryption trust check popping up with some 'false' messages. Don't worry as this will not influence our experiment. \n",
    "\n",
    "You should now be able to find these txt file in HDFS. Open http://localhost:9870/ , Utilities -> Browse the file system.\n",
    "\n",
    "\n",
    "<img src=\"files/hdfsbooks.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Run the MapReduce job\n",
    "\n",
    "Finally we are ready to run our MapReduce job:  \n",
    "```\n",
    "mapred streaming \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-file reducer.py \\\n",
    "-file mapper.py \\\n",
    "-input /books/* \\\n",
    "-output /output-folder\n",
    "```\n",
    "\n",
    "This command allows us to use the streaming API from Hadoop. We need to pass each file used, the mapper and the reducer and finally the input files and the output folder (__the output folder must be a new non-already-existing folder__).  \n",
    "We can pass additional configuration parameters, namely we can ask Hadoop to use a certain number of reducers by setting the `numReduceTasks` command-line argument \n",
    "\n",
    "```\n",
    "mapred streaming \\\n",
    "-numReduceTasks 4 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-file reducer.py \\\n",
    "-file mapper.py \\\n",
    "-input /books/* \\\n",
    "-output /output-folder2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Download and plot the results\n",
    "The output of the MapReduce process has been stored in your remote cluster. You can now download the result file and plot the frequency of the 30 most common words in the dataset. \n",
    "\n",
    "By default, output files have the form `part-XXXX` where `XXXX` is the id of the specific mapper or reducer task.\n",
    "\n",
    "Refresh the HDFS browser, find the output directory that we specified, click on the output file -> download. You should be direct to a page. Copy the URL. For me, the URL is `http://231212fae531:9864/webhdfs/v1/output-folder/part-00000?op=OPEN&namenoderpcaddress=namenode:9000&offset=0`\n",
    "\n",
    "<img src=\"files/hdfsout.png\">\n",
    "\n",
    "<img src=\"files/hdfsdownload.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your output URL here\n",
    "RESULT_FILE_URL = 'XXX'\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve (RESULT_FILE_URL, \"results.txt\")\n",
    "print ('Done downloading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "print ('Plotting...')\n",
    "\n",
    "# Read input and sort by frequency. Keep only top 30.\n",
    "with open('results.txt', 'r') as csvfile:\n",
    "    for line in csvfile.readlines():\n",
    "        ##### TODO: read the word and count here #####\n",
    "        pass\n",
    "\n",
    "##### TODO: sort the word by frequecy #####\n",
    "\n",
    "\n",
    "##### TODO: draw a bar plot by filling the arguments below #####\n",
    "# Generate plot\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.bar(...)\n",
    "plt.xticks(...)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reading word counts from results.txt, find the 30 most frequent words in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 A Glance to Yarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YARN is a complex framework that handles resource management on the cluster. You will get to know more about it during the YARN lecture, but for now we will just use it to monitor our MapReduce job live. \n",
    "\n",
    "To start monitor the cluster, let's open another terminal connecting to the ResourceManager of the cluster (keep your previous NameNode terminal alive). \n",
    "\n",
    "``` bash\n",
    "docker exec -it resourcemanager /bin/bash\n",
    "```\n",
    "\n",
    "Let's see what is the situation with the cluster now.\n",
    "\n",
    "``` bash\n",
    "yarn node -list\n",
    "```\n",
    "\n",
    "Currently, there should be 0 container running as we have already finished our task. Let's start the task again to see what happens. In the NameNode terminal: \n",
    "\n",
    "```\n",
    "mapred streaming \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-file reducer.py \\\n",
    "-file mapper.py \\\n",
    "-input /books/* \\\n",
    "-output /output-folder3\n",
    "```\n",
    "\n",
    "In the meantime, on the ResourceManager terminal, keep repeating the command (use arrow up on your keyboard to repeat the command). You should be find that the number of container raises at runtime, and return to 0 after the task finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of the hands-on part\n",
    "\n",
    "Shut down docker and delete image if you don't need to revisit this experiment anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding MapReduce's execution model\n",
    "\n",
    "For each of the following statements, state whether it is *true* or *false* and briefly explain why.\n",
    "\n",
    "1. Each mapper task must generate the same number of key/value pairs as its input had.\n",
    "2. The TaskTracker is responsible for scheduling mapper and reducer tasks and make sure all nodes are correctly running.\n",
    "3. Mappers input key/value pairs are sorted by the key.\n",
    "4. MapReduce splits might not correspond to HDFS block.\n",
    "5. One single Reducer task is applied to all values associated with the same key.\n",
    "6. Multiple Reducer tasks can be assigned pairs with the same value.\n",
    "7. In Hadoop MapReduce, the key-value pairs a Reducer outputs must be of the same type as its input pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A comprehension task\n",
    "Conceptually, a map function takes in input a kay-value pair and emits a list of key-values pairs, while a reduce function takes in input a key with an associated list of values and returns a list of values or key-value pairs. Often the type of the final key and value is the same of the type of the intermediate data:\n",
    "\n",
    "- map     `(k1,v1) -> list(k2,v2)`\n",
    "- reduce  `(k2,list(v2))-> list(k2, v2)`\n",
    "\n",
    "Analyze the following Mapper and Reducer functions, written in pseudo-code, and answer the questions below.\n",
    "\n",
    "```js\n",
    "function map(key, value)\n",
    "  emit(key, value);\n",
    "```\n",
    "\n",
    "```js\n",
    "function reduce(key, values[])\n",
    "  z = 0.0\n",
    "  for value in values:\n",
    "    z += value\n",
    "  emit(key, z / values.length())\n",
    "```\n",
    "\n",
    "** Questions **\n",
    "1. Explain what is the result of running this job on a list of pairs with type ([string], [float]).\n",
    "2. Could you use this reduce function as combiner as well? Why or why not?\n",
    "3. If your answer to the previous question was *yes*, does the number of different keys influences the effectiveness of the combiner? If you answer was *no*, can you change (if needed) map and reduce functions in such a way the new reducer the can be used as combiner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own Exploration\n",
    "\n",
    "Imagine you are given a dataset with the temperatures and precipitations around the world for a given day.  \n",
    "The initial KV pairs consists of `(line number in the file) -> (country,station_id,avg_temperature,mm_of_rain)`.  \n",
    "You can assume that all station IDs are distinct.   \n",
    "Write a MapReduce job (using pseudocode as seen in task 3) for each of the following problems, also state whether it is possible to use a combiner to speed up the computation.\n",
    "\n",
    "1. Find for each country except the UK the maximum avg_temperature  \n",
    "2. Find for each country the station_id with the maximum avg_temperature  \n",
    "3. Find for each country the total amount of mm_of_rain but only for countries for which the total is greater than 100mm  \n",
    "4. Find for each country the total amount of mm_of_rain from stations in which it rained more than 10mm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
