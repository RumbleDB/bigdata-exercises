{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <center>Big Data for Engineers &ndash; Exercises</center>\n",
    "## <center>Spring 2022 &ndash; Week 5 &ndash; ETH Zurich</center>\n",
    "## <center>Wide Column Stores - HBase</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will consist of two main parts: \n",
    "* Hands-on practice with your own HBase cluster running in Docker\n",
    "* Architecture of HBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 &mdash; Creating and using an HBase cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to touch HBase! You will create, fill with data, and query an HBase cluster running on Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: in the HBase folder [ if you encounter any issues with Option 1, try Option 2 below]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the following to set up an HBase cluster using Docker:\n",
    "\n",
    "\n",
    "0. Please first get the folder from Moodle containing all the necessary files that configure Docker. Please also open Docker, you should see that docker is active with green on the UI of Docker desktop. <br>\n",
    "\n",
    "\n",
    "1. In the command line, navigate into your HBase folder using 'cd' command and instantiate the cluster by running:<br>\n",
    "<b>docker-compose -f docker-compose.yml up -d </b><br>\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/Jt9FFHTNqRB3Pok/download\" style=\"width:500px;\"><br>\n",
    "After a few minutes you should get this output in the command line and the UI will display each container's status:\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/AWGzyitKNgs6zjZ/download\" style=\"width:500px;\"><br>\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/f5GFPirawb6yAjQ/download\" style=\"width:500px;\"><br>\n",
    "\n",
    "\n",
    "2. Copy the <b>.csv</b> data file from your local system to the docker container (the hbase-master) where hbase is running using the command:<br>\n",
    "<b> docker cp enwiki-20200920-pages-articles-multistream_small.csv hbase-master:/ </b> <br>\n",
    "This will be used in Exercise 2. <br>\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/6EpH9a89baLZsFY/download\">\n",
    "<br>\n",
    "\n",
    "\n",
    "3. Access to your container's bash by running the following command:<br>\n",
    "<b>docker exec -it hbase-master /bin/bash</b><br>\n",
    "Once in the bash, you can list your files and check the presence of the .csv data file: <br>\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/BRdiKgcax4N6bRf/download\"> <br>\n",
    "\n",
    "\n",
    "4. We will use the <b>.csv</b> file later to populate our database. For now let's explore the basics of HBase in this playground. To start, run the following command in the container's bash:<br>\n",
    "<b>hbase shell</b>\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/NP5DiPKGwnE33CY/download\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: in the exercise05 folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the command line, navigate into the folder containing this jupyter notebook and run the same command as on the first screenshot above. (Remember you should have docker desktop runnning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cloud.inf.ethz.ch/s/2ZFcGMQ6qKe26Bk/download\" > <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few moments, you will see all containers running:\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/x2D87JH3F5PrQwg/download\" style=\"width:600px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now run the commands similarly to the first option, just replace `hbase-master` with `exercise05_hbase-master_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with your HBase cluster using the shell\n",
    "\n",
    "In this task we will go through some basic HBase commands, in preparation for the exercise where we will import a dataset from a .csv file and run queries against it.\n",
    "\n",
    "Open the HBase shell by running the following command:\n",
    "\n",
    "**`hbase shell`**\n",
    "\n",
    "Let's say we want to create an HBase table that will store sentences adhering to the structure <b>subject-verb-object</b> (e.g., \"I eat mangoes\", \"She writes books\") in different languages. Here is a schema that we may use:\n",
    "\n",
    "Table name = `sentences`\n",
    "* Column family: `words`\n",
    "  * column: `subject`\n",
    "  * column: `verb`\n",
    "  * column: `object`\n",
    "* Column family: `info`\n",
    "  * column: `language`\n",
    "\n",
    "With the following command we can create such a table (a description of HBase shell commands is available [here](https://learnhbase.wordpress.com/2013/03/02/hbase-shell-commands/)):\n",
    "\n",
    "**`create 'sentences', 'words', 'info'`**\n",
    "\n",
    "You can see the schema of the table with this command:\n",
    "\n",
    "**`describe 'sentences'`**\n",
    "\n",
    "Let's insert some sentences into our table. We will put data cell by cell using the following command format <br>\n",
    "    `put <table>, <rowId>, <columnFamily:columnQualifier>, <value>`:\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:subject', 'I'`**\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:verb', 'drink'`**\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:object', 'coffee'`**\n",
    "\n",
    "Now, let's try to query this sentence from the table:\n",
    "\n",
    "**`get 'sentences', 'row1'`**\n",
    "\n",
    "You should see output similar to this:\n",
    "\n",
    "```\n",
    "COLUMN                                   CELL\n",
    " words:object                            timestamp=1602785046682, value=coffee\n",
    " words:subject                           timestamp=1602785045625, value=I\n",
    " words:verb                              timestamp=1602785045849, value=drink\n",
    "3 row(s) in 0.0910 seconds\n",
    "```\n",
    "\n",
    "As you can see, HBase shell returns data as key-value pairs rather than as rows literally. You may also notice that the lines are lexicographically sorted by the key, which is why \"subject\" appears after \"object\" in the list.\n",
    "\n",
    "I don't know about you, but I like tea more than coffee, so let me update our sentence...\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:object', 'tea'`**\n",
    "\n",
    "As you can see, we are using the same `put` command to *update* a cell. But remember that HBase does not actually update cells in place&mdash;it just inserts new versions instead. If you now run the query again, you will see the new data:\n",
    "\n",
    "**`get 'sentences', 'row1'`**\n",
    "\n",
    "returns:\n",
    "\n",
    "```\n",
    "COLUMN                                   CELL\n",
    " words:object                            timestamp=1602785160890, value=tea\n",
    " words:subject                           timestamp=1602785045625, value=I\n",
    " words:verb                              timestamp=1602785045849, value=drink\n",
    "3 row(s) in 0.0200 seconds\n",
    "```\n",
    "\n",
    "We actually wanted to store sentences in different languages, so let's first set the language for the existing one:\n",
    "\n",
    "**`put 'sentences', 'row1', 'info:language', 'English'`**\n",
    "\n",
    "Note that we are now inserting a value into a different column family but for the same row. Verify with a `get` that this took effect. \n",
    "\n",
    "Now, let's add a sentence in another language (note that we are using another rowID now&mdash;`row2`):\n",
    "\n",
    "**`put 'sentences', 'row2', 'words:subject', 'Ich'`**\n",
    "\n",
    "**`put 'sentences', 'row2', 'words:verb', 'trinke'`**\n",
    "\n",
    "**`put 'sentences', 'row2', 'words:object', 'Tee'`**\n",
    "\n",
    "**`put 'sentences', 'row2', 'info:language', 'Deutsch'`**\n",
    "\n",
    "Let's check that we indeed have 2 rows now:\n",
    "\n",
    "**`count 'sentences'`**\n",
    "\n",
    "Now, let's query all rows from the table:\n",
    "\n",
    "**`scan 'sentences'`**\n",
    "\n",
    "This, indeed, returns all two rows, in key-value format as before.\n",
    "\n",
    "\n",
    "Of course, you can also scan by column family for column:\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'info'}`**\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'info:language'}`**\n",
    "\n",
    "You can also scan by row ranges (note min incl., max excl.):\n",
    "\n",
    "**`scan 'sentences', {STARTROW=>'row1', ENDROW=>'row3'}`**\n",
    "\n",
    "It is possible to also do some filtering in queries:\n",
    "\n",
    "**`scan 'sentences', {FILTER => \"ValueFilter(=, 'binary:English')\"}`**\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'words:subject', FILTER => \"ValueFilter(=, 'substring:I')\"}`**\n",
    "\n",
    "**`scan 'sentences', {COLUMNS => 'words:object', ROWPREFIXFILTER => 'row'}`**\n",
    "\n",
    "What if we want to store a sentence that also contains an adjective, in addition to the subject, verb, and object? <br>\n",
    "This is not a problem with HBase, because we can create new columns inside *existing* column families on the fly:\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:subject', 'Grandma'`**\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:verb', 'bakes'`**\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:adjective', 'delicious'`**\n",
    "\n",
    "**`put 'sentences', 'row3', 'words:object', 'cakes'`**\n",
    "\n",
    "This row now has more columns in the `words` column family than others:\n",
    "\n",
    "**`get 'sentences', 'row3'`**\n",
    "\n",
    "**`scan 'sentences'`**\n",
    "\n",
    "We can also add new columns to existing rows:\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:adjective', 'hot'`**\n",
    "\n",
    "**`get 'sentences', 'row1'`**\n",
    "\n",
    "Let's see what happens if you update a value in an existing column:\n",
    "\n",
    "**`put 'sentences', 'row1', 'words:adjective', 'cold'`**\n",
    "\n",
    "You should notice that the time stamp of the column `words:adjective` has been updated.\n",
    "\n",
    "When you are done with the queries, simply type `exit` to quit the hbase shell.\n",
    "\n",
    "Note: to drop a table in HBase, first `disable 'table_name'`, then `drop 'table_name'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 &mdash; The Wikipedia dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will see how HBase will handle a large dataset, as well as learn about the filters and caching in HBase.\n",
    "\n",
    "The provided dataset comprises of metadata information of articles from the English Wikipedia. You will see the following variables in the .csv file: \n",
    "\n",
    "| Variable | Meaning | Sample value|\n",
    "|:------|:---------------|:---------------|\n",
    "|`page_id`| the page id in the enwiki data dump |1000108|\n",
    "|`page_title`| the page id in the enwiki data dump |AEG Z.6|\n",
    "|`page_ns`| page namespace|0|\n",
    "|`revision_id`| the id of revision to the article |16782282|\n",
    "|`timestamp`| the time a contributor makes a revision |2004-09-19T23:44:33Z|\n",
    "|`contributor_id`| the id of contributor |8817|\n",
    "|`contributor_name`| the name of contributor |Rlandmann|\n",
    "|`bytes`| bytes in revision |21|\n",
    "\n",
    "\n",
    "We use the `wiki_small` dataset (about 85MB in .csv) in this assignment because it takes shorter time (about 5-10 minutes) to load into HBase.\n",
    "\n",
    "Based on the variable description in the table above, we can categorize the variables into 2 categories: <br>\n",
    "(1) some variables are about a page;<br>\n",
    "(2) some variables are about an author/contributor. <br>\n",
    "Let us now create the schema in HBase with two column families, `page` and `author`:\n",
    "\n",
    "**`hbase shell`**\n",
    "\n",
    "**`create 'wiki_small', 'page', 'author'`**\n",
    "\n",
    "After the table is created, we need to exit the HBase shell and return back to the container's bash:\n",
    "\n",
    "**`exit`**\n",
    "\n",
    "Now we need to populate both tables with data. We will use the [ImportTsv](https://hbase.apache.org/book.html#importtsv) utility of HBase. \n",
    "\n",
    "Recall that you should run this command into the container's bash.\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/Pg5g3z9Hsm7ybdT/download\"><br>\n",
    "\n",
    "Populate the table `wiki_small` by running the following command. We need to specify which column in the csv maps to which column in the HBase table. Note that we make `page_id` into the `HBASE_ROW_KEY`. Note that these commands print a lot of messages, but they are mostly informational with occasional non-critical warnings; unless something goes wrong, of course :). The commands will also report some \"Bad Lines\", but you can safely ignore this&mdash;some lines may contain illegal characters and be dropped, but most of the data is in good shape.\n",
    "\n",
    "**`hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns=\"HBASE_ROW_KEY,page:page_title,page:page_ns,page:revision_id,author:timestamp,author:contributor_id,author:contributor_name,page:bytes\" wiki_small enwiki-20200920-pages-articles-multistream_small.csv`**\n",
    "\n",
    "Note here how we specify the mappings between the **.csv columns** and the **family:column** in the HBase table.\n",
    "\n",
    "You can count how many rows there are using this command from your head node's shell: <br>\n",
    "**`hbase org.apache.hadoop.hbase.mapreduce.RowCounter 'wiki_small'`**\n",
    "\n",
    "\n",
    "Now let's go into HBase shell again and run some queries against the `wiki_small` table. We will look at some of the filters listed by HBase if you run `show_filters` in an HBase shell, e.g., `PrefixFilter(), ValueFilter(), SingleColumnValueFilter()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks to do with the table `wiki_small`\n",
    "\n",
    "1. How does HBase index the row keys? \n",
    "We choose `page_id` in the original table to be the row keys in the HBase table. Run these two queries and what do you observe? What can we say about row key indexing based their results? \n",
    "\n",
    "    **`scan 'wiki_small', {STARTROW=>'100009', ENDROW=>'100011'}`** <br>\n",
    "    **`scan 'wiki_small', {STARTROW=>'100015', ENDROW=>'100016'}`**\n",
    "\n",
    "1. Write the following queries:\n",
    "  1. Select all article titles and author names where the row name starts with '`1977`'.\n",
    "  1. Select all article titles and author names where the author contains the substring '`tom`'. \n",
    "  \n",
    "1. Write the following queries:\n",
    "  1. Return the number of articles from 2017.\n",
    "  1. Return the number of articles that contain the word `Sydney` on them. Please discuss different possibilities to formulate this query. \n",
    "  \n",
    "1. Execute your queries more than once and observe the query execution times\n",
    "1. What are the advantages and disadvantages of pure row stores?\n",
    "1. What are the advantages and disadvantages of pure column stores?\n",
    "1. What are the advantages and disadvantages of wide column stores?\n",
    "1. What are the advantages and disadvantages of denormalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 &mdash; Architecture of HBase\n",
    "\n",
    "In the previous tasks, we have seen HBase in action. Let us now take a look at the internal architecture of HBase. You may want to consult the lecture slides when solving these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1 &mdash; Inside a RegionServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will see how a RegionServer in HBase would execute a query.\n",
    "\n",
    "Imagine that we have an HBase table called '`phrases`', which has the following schema:\n",
    "\n",
    "* Column family: `words`\n",
    "  * column: A\n",
    "  * column: B\n",
    "  * column: C\n",
    "  * (potentially also columns D, E, F, etc.)\n",
    "\n",
    "Thus, the table has only one column family. Each column in this family holds one word.\n",
    "\n",
    "Recall from the lecture slides that keys in HBase have the following structure:\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/jYJmNrzReEXsPJJ/download\" width=\"70%\">\n",
    "\n",
    "We will make some simplifications to keys format to avoid excessive clutter in this exercise. Since the table in this exercise has only one column family, we will omit it from the key and only specify the column name (A,B,C, ...). We will also omit the length fields and the \"key type\" field. The timestamp field in this exercise will contain integers from 1 to 10 (in reality, the timestamp would contain the number of milliseconds). Thus, keys that will be used in this exercise consist of three fileds: **row, column, timestamp**.\n",
    "\n",
    "### Tasks to do\n",
    "\n",
    "State which Key-Value pairs will be returned by each of the following queries. Consider that you are using the HBase shell syntax which you have already seen in the first exercise. Also assume that the HBase instance is configured to return only the latest version of a cell.\n",
    "\n",
    "1. `get 'phrases', '278'`\n",
    "1. `get 'phrases', '636'`\n",
    "1. `get 'phrases', '593'`\n",
    "1. `get 'phrases', '640'`\n",
    "1. `get 'phrases', '443'`\n",
    "\n",
    "To answer this question, use the diagram below, which represents the state of a RegionServer responsible for the row region in the range of row IDs 100&ndash;999, which is the region into which all these queries happen to fall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cloud.inf.ethz.ch/s/XsqbGig8SHEKdHo/download\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can format your answer for this exercise as follows\n",
    "\n",
    "1. get 'phrases', 'row_id' \n",
    "\n",
    "| Row | Column | Timestamp | Value | Where it came from (which HFile) |\n",
    "|:-----:|:-----:|:-----:|:-------|:--------------------:|\n",
    "||||||\n",
    "||||||\n",
    "||||||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2 &mdash; Building an HFile index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HBase uses Bloom filters, stored in the metadata of each HFile, in order to discard all get requests which query data not stored in the HFile. However when performing a get for which the bloom filter returns positive, the RegionServer needs to check its MemStore and all HFiles for the existence of the requested key. In order to avoid scanning HFiles entirely, HBase uses index structures to quickly skip to the position of the *HBase block* which may hold the requested key. Note HBase block is not to be confused with HDFS block and the underlying file system block, see [here](https://blog.cloudera.com/hbase-blockcache-101/#3) for a good discussion. HBase blocks come in 4 varieties: DATA, META, INDEX, and BLOOM.\n",
    "\n",
    "By default, each *HBase block* is 64KB (configurable) in size and always contains whole key-value pairs, so, if a block needs more than 64KB to avoid splitting a key-value pair, it will just grow. \n",
    "\n",
    "In this task, you will be building the index of an HFile. __For the purpose of this exercise__, assume that each HBase block is 40 bytes long, and each character in keys and values is worth 1 byte: for example, the first key-value pair in the diagram below is worth $3 + 1 + 1 + 6 = 11$ bytes. Below this diagram you will find a table for you to fill in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cloud.inf.ethz.ch/s/96M88qHqpgjGG2c/download\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the contents of the HFile above, you need to populate the index, following the approach described in the lecture slides. Use the following table (again, you can edit it by double-clicking). Use as many or as few rows as you need.\n",
    "\n",
    "| RowId | Column | Version |\n",
    "|-------|--------|---------|\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "|       |        |         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
