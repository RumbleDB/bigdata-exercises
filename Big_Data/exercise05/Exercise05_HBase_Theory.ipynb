{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <center>Big Data &ndash; Exercises</center>\n",
    "## <center>Autumn 2025 &ndash; Week 5 &ndash; ETH Zurich</center>\n",
    "## <center>Wide Column Stores - HBase</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this exercise you will analyse some important architectural components of HBase. The following is a brief summary of the exercises and their corresponding topics:\n",
    "\n",
    "[Exercise 1: Region Servers](#exercise_1)\n",
    "\n",
    "[Exercise 2: HFile Indices](#exercise_2)\n",
    "\n",
    "[Exercise 3: Bloom Filters](#exercise_3)\n",
    "\n",
    "[Exercise 4: Log-Structured Merge-Trees](#exercise_4)\n",
    "\n",
    "[Exercise 5: Write-ahead Logs](#exercise_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise_1'></a>\n",
    "## Exercise 1 &mdash; Inside a RegionServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will take on the role of a RegionServer in HBase and execute several `get` queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting\n",
    "Imagine that we have an HBase table called '`phrases`', which has a single column family `words` with 3 columns: A, B and C. Each of these columns stores a single word.\n",
    "\n",
    "Recall from the lecture slides that keys in HBase have the following structure:\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/XjO0LM1r6L10kT3/download\" width=\"60%\">\n",
    "<a id=\"simple_keys\"></a>\n",
    "\n",
    "To avoid excessive clutter in this exercise, we simplify the structure as follows:\n",
    "- We omit the `column family` from the key, since the table has a single column family.\n",
    "- We omit the `key length` and `column family length` fields.\n",
    "- We omit the `key type` field, i.e. we assume no records can be deleted. \n",
    "- Instead of a number of milliseconds, the `timestamp` field will contain integers from 1 to 10, where a larger integer signifies a latter version.\n",
    "\n",
    "Thus, keys that will be used in this exercise consist of only three fileds: **row, column, timestamp**. This is pictured below:\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/82LbiSQCyTnMr5i/download\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing queries\n",
    "Imagine that you are the RegionServer responsible for storing the region with row keys `100` - `999`.\n",
    "\n",
    "The diagram below represents your state at the time of execution:\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/mbc7boFDDCo2V9D/download\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State which Key-Value pairs will be returned by each of the following queries. Assume that the HBase instance is configured to return only the latest version of a cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `get 'phrases', '278'`\n",
    "\n",
    "| Row | Column | Timestamp | Value | Where it came from (which HFile) |\n",
    "|:---:|:------:|:---------:|:------|:--------------------------------:|\n",
    "|     |        |           |       |                                  |\n",
    "|     |        |           |       |                                  |\n",
    "|     |        |           |       |                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `get 'phrases', '636'`\n",
    "\n",
    "| Row | Column | Timestamp | Value | Where it came from (which HFile) |\n",
    "|:---:|:------:|:---------:|:------|:--------------------------------:|\n",
    "|     |        |           |       |                                  |\n",
    "|     |        |           |       |                                  |\n",
    "|     |        |           |       |                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `get 'phrases', '593'`\n",
    "\n",
    "| Row | Column | Timestamp | Value | Where it came from (which HFile) |\n",
    "|:---:|:------:|:---------:|:------|:--------------------------------:|\n",
    "|     |        |           |       |                                  |\n",
    "|     |        |           |       |                                  |\n",
    "|     |        |           |       |                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `get 'phrases', '640'`\n",
    "\n",
    "| Row | Column | Timestamp | Value | Where it came from (which HFile) |\n",
    "|:---:|:------:|:---------:|:------|:--------------------------------:|\n",
    "|     |        |           |       |                                  |\n",
    "|     |        |           |       |                                  |\n",
    "|     |        |           |       |                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `get 'phrases', '443'`\n",
    "\n",
    "| Row | Column | Timestamp | Value | Where it came from (which HFile) |\n",
    "|:---:|:------:|:---------:|:------|:--------------------------------:|\n",
    "|     |        |           |       |                                  |\n",
    "|     |        |           |       |                                  |\n",
    "|     |        |           |       |                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise_2'></a>\n",
    "## Exercise 2 &mdash; Building an HFile index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When servicing a Read request, the RegionServer needs to check its MemStore and all HFiles for the existence of the requested key. In order to avoid scanning HFiles entirely, HBase uses sparse index structures to quickly skip to the position of the *HBase block* which *may* hold the requested key. By default, each *HBase block* is 64kB (but it is configurable) in size and always contains whole key-value pairs, so, if a block needs more than 64kB to avoid splitting a key-value pair, it will just grow. \n",
    "\n",
    "__For the purpose of this exercise we make the following assumptions:__,\n",
    "- Each HBase block is only 40 bytes long (instead of the usual 64kB).\n",
    "- The key of each key-value pair consists only of the row key, column qualifier and the version timestamp, as defined in the [previous exercise](#simple_keys).\n",
    "- Each character takes exactly 1 byte of memory\n",
    "  - For example, the first key-value pair in the diagram below would take $3 + 1 + 1 + 6 = 11$ bytes (3 bytes for `113`, 1 byte for `C`, 1 byte for `5`, 6 bytes for `little`.)\n",
    "\n",
    "Using this information, your task is to now build the index of the HFile below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://polybox.ethz.ch/index.php/s/Sj5PNFWcy8TbVOh/download\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate how the blocks will be \"assembled\", and then use the appropriate keys for the index.\n",
    "\n",
    "You can use the following table (again, you can edit it by double-clicking). Use as many or as few rows as you need.\n",
    "\n",
    "| RowId | Column | Version |\n",
    "|-------|--------|---------|\n",
    "|       |        |         |\n",
    "|       |        |         |\n",
    "\n",
    "What is the size of each block?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise_3'></a>\n",
    "## Exercise 3 &mdash; Bloom filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Exercise 2](#exercise_2) we built an index for an HFile containing data, and stored it in a separate index file. This index file was sorted on the row key. Each entry then mapped to the corresponding block of the HFile, whose first entry was the key-value pair with that key. \n",
    "\n",
    "Now, if we wanted to search for an arbitrary row key in this index, the best we could do is a binary search on the index, followed by reading 1 block from the HFile the index is for. But in the presence of many HFiles, we don't want to read all corresponding blocks, as that would greatly slow us down.\n",
    "\n",
    "**So, how can we optimize?**\n",
    "\n",
    "One way to do that is with **Bloom filters** - a space-efficient probabilistic data structure used by HBase to directly discard all read requests where query data is NOT stored in the HFile. Bloom Filters are stored in the metadata of each HFile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we did not have time to cover these filters in the lecture, they are actually crucial if we want to avoid disk reads. Bloom filters allow us to determine whether an element belongs to a set or not very quickly.\n",
    "\n",
    "The main component of these filters is __a bit array__ with all values initially set to $0$. Let $k$ be the length of this array.\n",
    "\n",
    "When a new element is inserted in the collection (in our case, the HFile), the following happens:\n",
    " - Its value is first run through a fixed number of hash functions, which output some numbers.\n",
    " - The locations in the bit array corresponding to the outputs from the hash functions modulo $k$ are set to 1.\n",
    "\n",
    "When we query for a certain value:\n",
    " - We hash its value using the same hash functions, and we get several numbers as output.\n",
    " - If the value has been previously inserted in the collection, then all the locations corresponding to the hash function outputs modulo $k$ will certainly have been set to 1.\n",
    " - On the contrary, if the element hasn't been inserted previously, then the locations might or might not have already been set to 1 by other elements in the collection.\n",
    "  \n",
    "Thus, if prior to accessing the collection, we run our value through the hash functions, check the locations corresponding to the outputs modulo $k$, and find any of them to be 0, __we are guaranteed that the element is not present in the collection__ (No False Negatives), and we don't have to waste additional time. If the corresponding locations are all set to 1, the element may or may not be present in the collection (possibility of False Positives) and we still have to check.\n",
    "\n",
    "For clarity, let us inspect the following sequence. Assume that we have 3 hash functions and $k = 12$. Remember that the bit array has all values initialized to $0$:\n",
    "1. We insert `John Smith` in the collection. The hash functions return the locations $1$, $2$ and $7$. We set those locations in the bit array to 1\n",
    "   \n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/iMEEGRSaVkRHjDG/download\" width=\"50%\">\n",
    "\n",
    "2. We insert `Mary Smith` in the collection. The hash functions return the locations $5$, $2$ and $6$. We set those locations in the bit array to 1.\n",
    "   \n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/GJQgLU8W258YsJl/download\" width=\"50%\">\n",
    "\n",
    "3. Now, we query the collection with the key `Albert Einstein`. The hash functions return the locations $3$, $5$ and $6$. While, the bit array is set to $1$ in locations $5$ and $6$, the value at location $3$ is still $0$. This means that `Albert Einstein` __cannot__ be present in the collection. Thus we do not have to check the it.\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/MAQM88My5yWdTps/download\" width=\"50%\">\n",
    "\n",
    "4. We query the collection with the key `Louis de Broglie`. The hash functions return the locations $2$, $6$ and $7$. All of the corresponding bits in the array are set  to $1$, even though the key is not present in the collection. This means the filter returned a false positive and we have to traverse the collection only to find out the key is not present in there.\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/xWrmQIPWV5iGj64/download\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen in [Exercise 1](#exercise_1), without bloom filters, HBase has to check all HFiles, along with the MemStore, when looking for a particular key. With Bloom filters, we can avoid checking entire HFiles.\n",
    "\n",
    "And now to the key point of this talk - how bloom filters are used in HBase. In short:\n",
    "\n",
    "__We associate each HFile with a bloom filter. Before looking inside a particular HFile, HBase first checks the requested key against the Bloom filter associated with that HFile. If it says that the key does not exist, the file is not read. If it says the key might be inside, we proceed as usual - binary search on the index, followed by retrieving and checking the corresponding HBlock.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now populate a bloom filter using a list of words. Then you will query it.\n",
    "\n",
    "A bloom filter requires several hash functions. To keep things easily computiable, for the purpose of this exercise, we define the following 3 hash functions:\n",
    "\n",
    "1. Given a word $x$, the value of the first hash function, $hash_1(x)$, is equal to the *first letter of the word*.\n",
    "2. Given a word $x$, the value of the second hash function, $hash_2(x)$, is equal to the *second letter of the word*.\n",
    "3. Given a word $x$, the value of the third hash function, $hash_3(x)$, is equal to the *third letter of the word*. \n",
    "\n",
    "For instance, for the word $w = $ `federal`:\n",
    "- $hash_1(w) =$ `f`\n",
    "- $hash_2(w) =$ `e`\n",
    "- $hash_3(w) =$ `d`\n",
    "\n",
    "As you have noticed, we use letters instead of numbers as outputs of these hash functions. Letters are also used as the locations in the bit array. This is to keep things simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, a bloom filter starts with a bit array which has value $0$ recorded for each possible output value. To avoid clutter, we omit the value of the bit array if it is $0$. Thus, the start state of the filter is as follows:\n",
    "| | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
    "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "|A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|Y|Z|\n",
    "\n",
    "Now, if we add the word \"`federal`\" to the Bloom filter, using the three hash functions that we have defined above, we get the following bit array:\n",
    "\n",
    "| | | |1|1|1| | | | | | | | | | | | | | | | | | | | |\n",
    "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "|A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|Y|Z|\n",
    "\n",
    "**Your task is as follows:**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Populate the following table with the outputs of the three hash functions (double-click the table to edit it):\n",
    "[**Collection A**]\n",
    "\n",
    "| Word    | $hash_1$    | $hash_2$    | $hash_3$    |\n",
    "|:--------|-------------|-------------|-------------|\n",
    "|round    |             |             |             |\n",
    "|sword    |             |             |             |\n",
    "|past     |             |             |             |\n",
    "|pale     |             |             |             |\n",
    "|nothing  |             |             |             |\n",
    "|darkness |             |             |             |\n",
    "|water    |             |             |             |\n",
    "|feet     |             |             |             |\n",
    "|thin     |             |             |             |\n",
    "|passage  |             |             |             |\n",
    "|corner   |             |             |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.__ Now, *add* each word from the list to the following Bloom filter (you can also double-click to edit it) [**Bloom Filter for Collection A**]\n",
    "\n",
    "| | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
    "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "|A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|Y|Z|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.__ For each word in the following list, state whether this Bloom filter reports it as belonging to the set or not. Where applicable, indicate whether the result is a false positive or not: [**Collection B**]\n",
    "\n",
    "| Word    | $hash_1$ | $hash_2$ | $hash_3$ | The Bloom filter says the word belongs to the set: (yes/no) | False positive (yes/no/NA) |\n",
    "|:--------|----------|----------|----------|:-----------------------------------------------------------:|:---------------------------|\n",
    "|sword    |          |          |          |                                                             |                            |\n",
    "|sound    |          |          |          |                                                             |                            |\n",
    "|psychic  |          |          |          |                                                             |                            |\n",
    "|pale     |          |          |          |                                                             |                            |\n",
    "|book     |          |          |          |                                                             |                            |\n",
    "|deaf     |          |          |          |                                                             |                            |\n",
    "|truss    |          |          |          |                                                             |                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the words that were flagged by the Bloom filter as **not** belonging to the set actually **do belong** to the set (a *false negative* outcome)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise_4'></a>\n",
    "## Exercise 4 &mdash; Log-structured merge-trees (LSM trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the optimisations we have done can be summarised as follows:\n",
    "\n",
    "1. **Sparse Index** (HFile index)  \n",
    "   A small, in-memory index that stores pointers. This helps quickly locate and jumpstart the approximate position of a key in the HFile before doing a more precise binary search.\n",
    "\n",
    "2. **Bloom Filter**  \n",
    "   A **probabilistic** data structure that can definitively say if a key **does not** exist in a given HFile (no false negatives). If the Bloom filter says \"not present,\" the HFile can be skipped entirely. If it says \"might be present,\" a normal lookup is done.\n",
    "\n",
    "\n",
    "In exercises 1 to 3 we have essentially seen methods to optimise reads on our HFiles. **But how are the HFiles created in the first place?**\n",
    "\n",
    "Essentially, the whole HBase system composed of MemStores, HFiles, and HFile indexes is an implementation of a very common pattern called [**log-structured merge-trees**](https://en.wikipedia.org/wiki/Log-structured_merge-tree). \n",
    "\n",
    "A **Log-Structured Merge (LSM) Tree** is a type of database index optimized for write-intensive workloads. It relies on maintaining an in-memory data structure (often a balanced binary search tree or skip list) for fast writes and periodically flushing that data to disk in sorted files.\n",
    "\n",
    "Before we explain how LSM Trees function, we give a mapping from common terminology to HBase specific naming:\n",
    "\n",
    "| Common name              | HBase name |\n",
    "|:-------------------------|:-----------|\n",
    "| MemTable                 | MemStore   |\n",
    "| SSTable                  | HFile      |\n",
    "| Tombstone                | Key type   |\n",
    "| Write-Ahead Log (WAL)    | HLog       |\n",
    "\n",
    "#### 1. **In-Memory Component (MemTable)**\n",
    "\n",
    "At the heart of an LSM Tree is an **in-memory balanced search tree** (e.g., a Red-Black Tree, AVL Tree, or sometimes a skip list). This structure, commonly called a **MemTable**, provides:\n",
    "\n",
    "- **Fast writes:** $\\mathcal{O}(\\log{n})$ insertion/update in memory.\n",
    "- **Fast reads on recent data:** $\\mathcal{O}(\\log{n})$ lookups for keys currently in memory.\n",
    "\n",
    "However, this approach has two main issues:\n",
    "\n",
    "1. **Lack of Durability**  \n",
    "   If the system crashes, everything in memory is lost.\n",
    "\n",
    "2. **Limited Memory and Excessive Growth**  \n",
    "   The in-memory tree can grow very large and must eventually be offloaded to disk to free up RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the above reasons, we have to periodically flush the data to disk.\n",
    "\n",
    "#### 2. **Flushing to SSTables**\n",
    "\n",
    "When the in-memory tree **exceeds a certain size** threshold, the database **flushes** it to disk, creating an **SSTable (Sorted String Table)**. This is done as follows:\n",
    "\n",
    "1. Perform an **in-order traversal** of the balanced tree to produce a sorted list in $\\mathcal{O}(n)$ time.  \n",
    "2. Write this sorted list to a **new SSTable file** on disk.  \n",
    "3. **Clear** the in-memory tree (MemTable) to accept new writes.\n",
    "\n",
    "**SSTables are immutable** after creation. Future changes to the same keys appear in newer SSTables.\n",
    "\n",
    "Over time, many SSTables may accumulate on disk. **Reads** must check:\n",
    "\n",
    "1. **In-Memory Tree**: $\\mathcal{O}(\\log{n})$ lookup. In case of a miss, we then proceed to check On-Disk.\n",
    "2. **On-Disk SSTables**: Search from **newest** to **oldest**. Each SSTable is sorted, so it can be searched with **binary search** (O(log n) per SSTable).\n",
    "\n",
    "If a key is found in a recent SSTable, there is no need to check older SSTables. Because SSTables are immutable, a newer table always has the most up-to-date version of a key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. **Deletions via Tombstones**\n",
    "\n",
    "As we have mentioned, our SSTables are immutable once written to disk. To handle deletions, LSM Trees do not immediately remove old records. Instead, a **tombstone** (a special marker) is written for a key that is deleted. The tombstone indicates that any earlier entries for that key should be ignored.\n",
    "\n",
    "- **Old SSTables** still contain the key.  \n",
    "- **The newer SSTable** has the tombstone, effectively overriding the older version.\n",
    "\n",
    "A tombstone is similar to the idea of a **“soft delete”** from the relational database world. When we delete data, HBase does not delete it right away, but associates a tombstone with it, instead. In other words, a tombstone is a marker that is kept to indicate data that has been deleted. When we execute a delete operation it’s instead treated as an insert operation that places a tombstone on the value. Tombstones are removed as part of major compaction: during a major compaction, any row with an expired tombstone will not be propagated further.\n",
    "\n",
    "Tombstones are a solution for deletes on LSM Trees, but they cause the following problems:\n",
    "\n",
    "1. As a tombstone itself is a record, it takes storage space. Hence, it should be kept in mind that **upon deletion, the application will end up increasing the data size** instead of shrinking it. Furthermore, if there are a lot of tombstones, the available storage for the application could be substantially reduced.\n",
    "2. When a table accumulates many tombstones, read queries on that table could become slow and can cause serious performance problems like timeouts. This is because we have to read much more data until an actual major compaction happens and removes the tombstones (and major compactions happen very infrequently, in the order of only once a week)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4. **Compaction**\n",
    "\n",
    "Since SSTables are immutable, over time:\n",
    "\n",
    "- **Tombstoned keys** accumulate.  \n",
    "- **Multiple versions** of the same key may exist in different SSTables.\n",
    "\n",
    "**Compaction** merges these SSTables, with the following procedure:\n",
    "\n",
    "1. **Merge Sort** approach: Because each SSTable is sorted, merging them is **$\\mathcal{O}(n)$** overall (where $n$ is the combined size of the SSTables).  \n",
    "2. **Discard old versions and tombstones** during the merge, creating a new, consolidated SSTable.  \n",
    "3. **Remove the original SSTables**.\n",
    "\n",
    "Compaction **reduces storage overhead** and **improves read performance** by minimizing the number of files to check.\n",
    "\n",
    "In HBase, there are two types of compaction. \n",
    "- **Minor Compactions** usually merge a small number of HFiles in a Region, and they do not remove tombstones.\n",
    "- **Major Compactions** merge **all** HFiles in a Region, and therefore can safely remove tombstones. As the operation is extremely expensive, it is usually ran infrequently (by default every seven days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Your Task**\n",
    "\n",
    "In summary, an LSM tree is highly efficient in applications using wide column storage such as HBase, Cassandra, BigTable, LevelDB, where insertions in memory happen quite often.\n",
    "\n",
    "The following figure is from the HBase Guide book where we see how a multipage block is merged from the in-memory tree into the next on-disk tree. Trees in the store files are arranged similar to B-trees. Merging writes out a new block with the combined result. Eventually, the trees are merged into the larger blocks.<br>\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/4o2fzmVJVbV7U1j/download\" width=\"60%\">\n",
    "\n",
    "\n",
    "Inserting data into the LSM Tree:\n",
    "\n",
    "1. When a write comes, it is inserted in the memory-resident MemStore.\n",
    "2. When the size of the MemStore exceeds a certain threshold, it’s flushed to disk.\n",
    "3. As MemStore is already sorted, creating a new HFile from it is efficient ($\\mathcal{O}(n)$, with $n$ the total number of key-value pairs in the tree).\n",
    "4. Old HFiles are periodically compacted together to save disk space and reduce fragmentation of data.\n",
    "\n",
    "Reading data from the LSM Tree:\n",
    "\n",
    "1. A given key is first looked up in the MemStore.\n",
    "2. Using a hash index, it’s searched for in one or more HFiles, depending on the status of the compaction.\n",
    "\n",
    "We will now walk through a concrete exercise to understand how LSM trees work in HBase. Imagine we have a client who is writing to HBase. The client also occasionally reads and deletes key-value pairs. The two types of storage that we consider in this task are the MemStore and the disk. For the purpose of this exercise, we ignore most parts of the key, apart from the row key and the timestamp. \n",
    "\n",
    "<img align=\"center\" width=\"50%\" src=\"https://polybox.ethz.ch/index.php/s/l1JTs41GhLE1RlK/download\"><br>\n",
    "\n",
    "The client requests the following operations (in this order):\n",
    "1. write the following key-value pairs: `(C,1), (B,2), (A,9), (A,109), (G,8), (D,67), (Z,0)`;\n",
    "2. read the value of key `A`;\n",
    "3. delete the key `Z`;\n",
    "4. write the following key-value pairs: `(S,100), (Z1,900), (A1,9), (A01,1), (Z1,850)`. \n",
    "\n",
    "Behind the scenes, flush and compaction operations are conducted to optimize read and write transfer. \n",
    "\n",
    "Please draw how the key-value pairs are stored in HBase and how HBase flushes and compacts files. Please note the following assumptions and simplifications:\n",
    "   - To simplify notation, we use `key[t] value` to denote that value `value` is associated with key `key` at time `t`. For example, when the client writes the key-value pair `(C,1)`, it is represented as `C[t1] 1`. Take a look at the image below for reference:\n",
    "\n",
    "<img align=\"center\" width=\"50%\" src=\"https://polybox.ethz.ch/index.php/s/tHMLCXeJZ2IW8Yp/download\"><br>\n",
    "\n",
    "   - The MemStore gets full after three key-value pairs have been inserted.\n",
    "   - When there exist two HFiles of size $n$ on the hard drive, we compact them.\n",
    "   - The timestamp is incremented by one __after__ every write. We start with a timestamp of `t1`.\n",
    "   - Only the latest version of a key is kept in the MemStore.\n",
    "   - When compacting, only the latest value of a key is preserved and tombstone key-value pairs are deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise_5'></a>\n",
    "## Exercise 5 &mdash; Write-Ahead Log (WAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, we have mentioned that machines can fail or restart at any time. What happens when the machine a program is running on loses power? The program might have atomicity and durability needs. Thus, it needs to know what the last thing it was doing was. Based on this information, it might then decide to redo or undo some of the operations it previously did to restore its state to a consistent one.\n",
    "\n",
    "So, how does the program retrieve information about what it was doing before the system crash? The answer is __logging__: persisting information about the operation on disk in append-only fashion either before or after performing it. In this exercise, we consider writing __before__ performing the operation. This type of logging is known as write-ahead logging and the corresponding log is called a **Write-ahead Log (WAL)** or **transaction log** or **commit log** or, in the case of HBase, an **HLog**. Writing to the WAL guarantees that if the machine crashes, the system will be able to recover and reapply the operation if necessary.  We have already seen one kind of WAL in this course - HDFS' edit log (or journal)!\n",
    "\n",
    "The key idea behind the WAL is that all modifications are persisted to a log file on disk, before they are actually applied in-memory. Each log entry contains enough information to redo or undo the modification. The log can be read on every restart to recover the previous state by replaying the necessary log entries. Using WAL results in a significantly reduced number of disk writes, because only the log file needs to be flushed to disk to guarantee that a transaction is committed, rather than every data file changed by the transaction.\n",
    "\n",
    "Each node, in a distributed environment, maintains its own log. A WAL is only appended to.\n",
    "\n",
    "<img align=\"center\" width=\"50%\" src=\"https://polybox.ethz.ch/index.php/s/un9OrcnS0Dw7GH2/download\"><br>\n",
    "\n",
    "To ensure durability, whenever a node in HBase receives a write request, it immediately writes the data to its WAL stored on HDFS before writing the data to a MemStore. This provides durability in the case of an unexpected shutdown, since all the data on the MemStore resides on volatile memory. On startup, the WAL will be replayed to repopulate the MemStore.\n",
    "\n",
    "<img align=\"center\" width=\"50%\" src=\"https://polybox.ethz.ch/index.php/s/5Tvn3SMUs11bhqG/download\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, to ensure durability, LSM Trees use a **Write-Ahead Log**:\n",
    "\n",
    "1. **Every write** (insert/update/delete) is **appended sequentially** to the WAL on disk.\n",
    "2. The same write is then applied to the in-memory tree.\n",
    "\n",
    "If the system crashes, the WAL can be **replayed** to reconstruct the in-memory tree. Because the WAL is **sequentially** written, it minimizes random I/O overhead.\n",
    "\n",
    "- **Pros:** Ensures durability with relatively fast sequential writes.  \n",
    "- **Cons:** Requires extra disk I/O for the log (although still quite efficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take a look at the configuration below. Note the following assumptions:\n",
    "- For simplicity, keys in this exercise consist only of a row key identifier and a timestamp (indicated as `TS`)\n",
    "- We use tombstone values for deleted entries (indicated as `tomb`).\n",
    "- The HLog file, if present, mirrors the contents of the MemStore at the given configuration.\n",
    "- When performing a read, only the latest value is returned.\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/28M8rKQ2DrqzPnG/download\"></img>\n",
    "\n",
    "Unfortunately, at time $t = 18$, the system crashes. After a short while, it recovers.\n",
    "\n",
    "__For each of the following keys indicate what value will be returned upon a read request of that key after the system's recovery, depending on whether the system kept a WAL before the crash or not.__\n",
    "\n",
    "Fill in the table below (You can double-click to edit it)\\\n",
    "_Hint: Use `empty` to indicate a missing return value_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Key      |  Value with HLog  | Value without HLog |\n",
    "|:--------:|:-----------------:|:------------------:|\n",
    "| bat      |                   |                    |\n",
    "| bear     |                   |                    |\n",
    "| fox      |                   |                    |\n",
    "| monkey   |                   |                    |\n",
    "| zebra    |                   |                    |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
