{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4jQWEtIgkeC"
   },
   "source": [
    "# <center>Big Data &ndash; Exercises &ndash; Solution</center>\n",
    "## <center>Fall 2022 &ndash; Week 7 &ndash; ETH Zurich</center>\n",
    "## <center>MapReduce</center>\n",
    "\n",
    "\n",
    "This exercise will consist of 2 main parts: \n",
    "* Hands-on practice with MapReduce on Azure HDInsight\n",
    "* Architecture and theory of MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_yesLkDgkeD"
   },
   "source": [
    "## 1. Setup a cluster\n",
    "\n",
    "### Create an Hadoop cluster\n",
    "\n",
    "Start the Hadoop cluster (in pseudo-distributed mode), similar to the HDFS exercise session, by running:\n",
    "\n",
    "```\n",
    "sudo docker-compose up\n",
    "```\n",
    "\n",
    "Wait for a couple minutes until the terminal no longer outputs startup logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmhNuxvDgkeE"
   },
   "source": [
    "## 2. Write a Word Count MapReduce job\n",
    "We want to find which are the most frequently-used English words. To answer this question, we prepared a big text files (1.7GB) where we concatenated thousands of books of the [Gutenberg dataset](https://zenodo.org/record/3360392#.Y2gbl3aZNO-). In this exercise, a smaller text file `gutenberg_x0.1.txt` is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LS4gpmdgkeE"
   },
   "source": [
    "### 2.1 Load the dataset\n",
    "\n",
    "The dataset we provide consists of a concatenation of thousands of books (`gutenberg.txt`). However we provide 2 versions, click the file below to download from Polybox. \n",
    "\n",
    "- [gutenberg_x0.1.txt](https://polybox.ethz.ch/index.php/s/Wi7M101WLUXtTgV) - a smaller dataset of about 190MB\n",
    "- [gutenberg.txt](https://polybox.ethz.ch/index.php/s/3FZxdA4wuPdNNQm) - the original dataset, 1.7GB\n",
    "\n",
    "Follow the steps below to set this dataset up in HDFS:\n",
    "\n",
    " - Transfer the dataset from you local computer to namenode container:\n",
    " \n",
    "```bash\n",
    "`docker cp ./gutenberg_x0.1.txt namenode:gutenberg_x0.1.txt`\n",
    "`docker cp ./gutenberg.txt namenode:gutenberg.txt`\n",
    "```\n",
    "\n",
    " - Log in into the NameNode container by launching a shell on it:\n",
    " \n",
    "```bash\n",
    "`docker exec -it namenode /bin/bash`\n",
    "```\n",
    "\n",
    " - Load the dataset into the HDFS filesystem:\n",
    " \n",
    "    With `ls -lh` you should see the 2 files mentioned above. These files are now in the \"local\" (remember, we are in containers) hard drive of your NmaeNode.\n",
    "\n",
    "    Upload the files into HDFS where they can be consumed by MapReduce:\n",
    "\n",
    "```bash\n",
    "`hdfs dfs -copyFromLocal *.txt /`\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYPPt_SBgkeF"
   },
   "source": [
    "### 2.2 Understand the MapReduce Java API\n",
    "\n",
    "We wrote a template project that you can use to experiment with MapReduce named `mapreduce.zip`. Store **on your local machine, not in the container** and unzip the following package.\n",
    "\n",
    "**Note: Before you docker cp, make sure you remove the old mapreduce directory from the NameNode**\n",
    "\n",
    "```bash\n",
    "`unzip mapreduce.zip`\n",
    "`docker cp mapreduce namenode:/mapreduce`\n",
    "```\n",
    "\n",
    "Now examine the content of the the `./mapreduce/src` folder. You will see one Java class:\n",
    "- *MapReduceWordCount*: a skeleton for a MapReduce job that loads data from file\n",
    "\n",
    "Start looking at *MapReduceWordCount*. You can see that the *main* method is already provided. Our `WordCountMapper` and `WordCountReducer` are implemented as classes that extend Hadoop's `Mapper` and `Reducer`. For this exercise, you only need to consider (and override) the `map()` method for the mapper and the `reduce()` method for the reducer.\n",
    "\n",
    "```java\n",
    "public class Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {\n",
    "\n",
    "    protected void map(KEYIN key, VALUEIN value, Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context context) {\n",
    "        context.write(key, value);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "```java\n",
    "public class Reducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {\n",
    "    protected void reduce(KEYIN key, Iterable<VALUEIN> values, Reducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context context) {\n",
    "        Iterator var4 = values.iterator();\n",
    "\n",
    "        while(var4.hasNext()) {\n",
    "            Object value = var4.next();\n",
    "            context.write(key, value);\n",
    "        }\n",
    "\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Consulting the [documentation](http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) if necessary, answer the following questions:\n",
    "\n",
    "1. What are possible types for `KEYIN, VALUEIN, KEYOUT and VALUEOUT`? Should `KEYOUT` and `VALUEOUT` for the Mapper be the same as `KEYIN` and `VALUEIN` for the Reducer?\n",
    "1. What is the default behavior of a MapReduce job running the base Mapper and Reducer above?\n",
    "1. What is the role of the object `Context`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HMuzr6NgkeG"
   },
   "source": [
    "### 2.3 Write and run your MapReduce wordcount\n",
    "\n",
    "Edit the provided skeleton and implement mapper and reducer to implement a word count. The goal is to know how many times each unique word appears in the dataset. You can consider each word as a sequence of characters separated by a whitespace, or implement a more sophisticated tokenizer if you wish.\n",
    "\n",
    "- Can you use your Reducer as Combiner? If so enable it by uncommenting the appropriate line in the `main` method.\n",
    "\n",
    "Once you are confident on your solution you can transfer it back to the container, compile it and run it, from the **mapreduce/src** folder.\n",
    "\n",
    "```\n",
    "javac *.java -cp $(hadoop classpath)\n",
    "jar cvf MapReduceWordCount.jar *.class \n",
    "```\n",
    "\n",
    "Inside the folder, you will now find `MapReduceWordCount.jar`.\n",
    "Run the map reduce job on the cluster using:\n",
    "\n",
    "```\n",
    "yarn jar MapReduceWordCount.jar MapReduceWordCount /gutenberg_x0.1.txt /tmp/results\n",
    "```\n",
    "\n",
    "To get the results after the job is done, simply copy to the local directory with the resuts:\n",
    "```\n",
    "hdfs dfs -copyToLocal /tmp/results\n",
    "```\n",
    "\n",
    "<br>The process is very similar to the one for HBase of last week. Answer the following questions:\n",
    "\n",
    "1. Run the MapReduce job on the cluster with the default configuration and 4 DataNodes using only `gutenberg_x0.1.txt` for now. *(Note: if you want to run your job again, you first need to delete the previous result folder because Hadoop refuses to write in the same location)*: \n",
    "```\n",
    "hdfs dfs -rm -r <path-to-hdfs-output-folder>\n",
    "```\n",
    "2. How many map and reduce tasks were created with the default configuration?\n",
    "3. Does it go faster with more reduce tasks? Uncomment and experiment with `job.setNumReduceTasks()`. What is the disadvantage of having multiple reducers? (Hint: check the format of your output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VfYK3G0gkeH"
   },
   "source": [
    "### 2.4. Plot the results\n",
    "By default, output files have the form `part-A-XXXX` where `A` is *r* or *m* to denote Reducer or Mapper outputs, and `XXXX` is the id of the specific mapper or reducer task.\n",
    "\n",
    "To plot the results, copy the file from the Docker container to your local machine, and then run the script below.\n",
    "\n",
    "```\n",
    "docker cp namenode:/path_to_results results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "print ('Plotting...')\n",
    "freq = {}\n",
    "\n",
    "# Read input and sort by frequency. Keep only top 30.\n",
    "# Read Reducer Output\n",
    "with open('./results/part-r-00000', 'rb') as csvfile:\n",
    "    for line in csvfile.readlines():\n",
    "        word, count = line.decode('UTF-8').split('\\t')\n",
    "        freq[word] = int(count)\n",
    "srt = sorted(freq.items(), key=operator.itemgetter(1), reverse=True)[:30]\n",
    "\n",
    "# Generate plot\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.bar(range(len(srt)), [x[1] for x in srt], align='center', color='#ba2121')\n",
    "plt.xticks(range(len(srt)), [x[0] for x in srt])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY3QmeZZgkeO"
   },
   "source": [
    "If everything is correct, the 3 most frequent words should be **`the`**, **`of`** and **`and`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1udbFcP6gkeO",
    "tags": []
   },
   "source": [
    "## 3. Performance comparison\n",
    "\n",
    "- Test your MapReduce with `gutenberg_x0.1.txt` and `gutenberg.txt`. For each test, **write down the running time**:\n",
    "\n",
    "```\n",
    "time yarn jar MapReduceWordCount.jar MapReduceWordCount /gutenberg_x0.1.txt /tmp/gut01/\n",
    "time yarn jar MapReduceWordCount.jar MapReduceWordCount /gutenberg.txt /tmp/gut1/\n",
    "```\n",
    "\n",
    "- For reference, it takes 34s for gutenberg_x0.1.txt and it takes 3m6s for gutenberg.txt.\n",
    "\n",
    "- We prepared a simple [wordcount program](./wordcount.py) in Python in this folder. Download it on your laptop (or the cluster NameNode) and test how long it takes to process these two datasets. **Annotate the times for the next exercise.**\n",
    "    \n",
    "```\n",
    "python wordcount.py < /pathtoyourfile/gutenberg_x0.1.txt\n",
    "python wordcount.py < /pathtoyourfile/gutenberg.txt\n",
    "```\n",
    "\n",
    "- For reference, it takes 5.724s for gutenberg_x0.1.txt in my local workstation and it takes 42.984s for gutenberg.txt. \n",
    "\n",
    "### 3.1 Plot\n",
    "Compare the performance of the MapReduce vs the single-thread implementation of the word count algorithm for two different input sizes. Fill the time in seconds in the code below to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7E3-w_U6gkeP"
   },
   "outputs": [],
   "source": [
    "\n",
    "size_input = [1.9*10e2, 1.7*10e3] # the input size in MB\n",
    "time_mapreduce = [0., 0.] # replace 0s with the time (in seconds) for the corresponding inputs\n",
    "time_locallaptop = [0., 0.] # replace 0s with the time (in seconds) for the corresponding inputs\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "# Import plot library\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot\n",
    "plt.figure(figsize=(18,9))\n",
    "plt.plot(size_input, time_mapreduce, '#f37626', label='MapReduce', linewidth=3.0)\n",
    "plt.plot(size_input, time_locallaptop, '#00b300', label='Local laptop', linewidth=3.0, linestyle='dashed')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Input size (MB)')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Wall-time comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ro81SXnlgkeR"
   },
   "source": [
    "### 3.2. Discussion\n",
    "\n",
    "We have run some more tests. Here we present the running time for 3 configurations on HDInsight, one workstation and one laptop. The figures below are indicative only, because the performance of every machine depends on several factors.\n",
    "\n",
    "- **MapReduce v1**: no combiner with default configuration (1 reducer)\n",
    "- **MapReduce v2**: no combiner with 8 reduce tasks\n",
    "- **MapReduce v3**: using combiner with default configuration (1 reducer)\n",
    "- **MapReduce v4**: using combiner with 8 reduce tasks\n",
    "- **Workstation**: using a local workstation (server)\n",
    "- **Laptop**: using a local laptop\n",
    "\n",
    "See our performance plot below:\n",
    "\n",
    "<img src=\"./figures/three_configurations_discussions.jpg\" style=\"width:800px;\">\n",
    "\n",
    "1. Which line corresponds to which setting?\n",
    "1. Which is faster, MapReduce on your cluster or a local wordcount implementation? Why?\n",
    "2. Based on your experiment, what input size is the approximate break-even point for time performance?\n",
    "3. Why MapReduce is not performing better than local computation for small inputs?\n",
    "4. How can you optimize the MapReduce performance for this job?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQoo-0jsgkeU"
   },
   "source": [
    "## 4. Querying JSON: The Language Confusion Dataset (optional)\n",
    "\n",
    "In this task, we will develop a MapReduce application that processes a dataset from the [language game](http://data.greatlanguagegame.com.s3.amazonaws.com/confusion-2014-03-02.tbz2).\n",
    "\n",
    "It contains rows of the following format:\n",
    "\n",
    "```json\n",
    "{\"target\": \"Turkish\",\n",
    " \"sample\": \"af0e25c7637fb0dcdc56fac6d49aa55e\",\n",
    " \"choices\": [\"Hindi\", \"Lao\", \"Maltese\", \"Turkish\"],\n",
    " \"guess\": \"Maltese\",\n",
    " \"date\": \"2013-08-19\",\n",
    " \"country\": \"AU\"}\n",
    "```\n",
    "\n",
    "Here, the `guess` field is what the user chose and the `target` field was the expected answer.\n",
    "\n",
    "### 4.1. Set up\n",
    "\n",
    "- Attach to the NameNode container:  `docker exec -it namenode /bin/bash`\n",
    "- Download the data **on your local machine, not in the container**: `curl -O http://data.greatlanguagegame.com.s3.amazonaws.com/confusion-2014-03-02.tbz2`\n",
    "- Extract the data: `tar -jxvf confusion-2014-03-02.tbz2`\n",
    "- Copy the data to the NameNode container: `docker cp confusion-2014-03-02 namenode:/confusion-2014-03-02`\n",
    "- Upload the data to HDFS: `hdfs dfs -put confusion-2014-03-02/confusion-2014-03-02.json /tmp/`\n",
    "\n",
    "### 4.2. Query implementation\n",
    "\n",
    "- You can start with the code provided in Task 3. Remove the old mapreduce directory, unzip `mapreduce.zip` and modify it accordingly.\n",
    "- On the NameNode container, remove the old mapreduce directory: `rm -rf /mapreduce_path`\n",
    "- The query to be implemented is:\n",
    "\n",
    "**Find the number of games where the guessed language is correct (i.e., guess equal to the target one) and that language is Russian.**\n",
    "\n",
    "- To parse a line of text, first add the next to the other imports:\n",
    "```java\n",
    "import com.google.gson.JsonObject;\n",
    "import com.google.gson.JsonParser;\n",
    "```\n",
    "\n",
    "- Then, you can use the following to parse and access json elements:\n",
    "\n",
    "```java\n",
    "...\n",
    "    JsonObject jsonObject = new JsonParser().parse(value.toString()).getAsJsonObject();\n",
    "    jsonObject.get(\"target\").getAsString();\n",
    "...\n",
    "```\n",
    "\n",
    "To compile and run the code, follow the same steps as above for Exercise 2.2.\n",
    "**Note: Make sure that the path to the json is the one on HDFS, not the one in the local container.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA3L4FhngkeW"
   },
   "source": [
    "## 5. Reverse engineering\n",
    "Conceptually, a map function takes an input a key-value pair and emits a list of key-values pairs, while a reduce function takes in input a key with an associated list of values and returns a list of values or key-value pairs. Often the type of the final key and value is the same of the type of the intermediate data:\n",
    "\n",
    "- map     `(k1,v1) --> list(k2,v2)`\n",
    "- reduce  `(k2,list(v2))--> list(k2, v2)`\n",
    "\n",
    "\n",
    "Analyze the following Map and Reduce functions, written in pseudo-code, and answer the questions below.\n",
    "\n",
    "```js\n",
    "function map(key, value)\n",
    "  emit(key, value);\n",
    "```\n",
    "\n",
    "```js\n",
    "function reduce(key, values[])\n",
    "  z = 0.0\n",
    "  for value in values:\n",
    "    z += value\n",
    "  emit(key, z / values.length())\n",
    "```\n",
    "\n",
    "**Questions**\n",
    "\n",
    "1. Explain what is the result of running this job on a list of pairs with type ([string], [float]).\n",
    "1. Write the equivalent SQL query.\n",
    "1. Could you use this reduce function as combine function as well? Why or why not?\n",
    "1. If your answer to the previous question was *yes*, does the number of different keys influences the effectiveness of the combine function? If you answer was *no*, can you change the map and reduce functions in such a way that the new reduce function can be used as combine function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jebWZzTlgkeX"
   },
   "source": [
    "## 6. True or False\n",
    "Say if the following statements are *true* or *false*, and explain why.\n",
    "\n",
    "1. Each map function must generate the same number of key/value pairs as its input had.\n",
    "1. The TaskTracker is responsible for scheduling map functions and reduce functions and make sure all nodes are correctly running.\n",
    "1. The input key/value pairs of map functions are sorted by the key.\n",
    "1. MapReduce splits might not correspond to HDFS block.\n",
    "1. One single Reduce function is applied to all values associated with the same key.\n",
    "1. Multiple Reduce functions can be assigned pairs with the same value.\n",
    "1. In Hadoop MapReduce, the key-value pairs a Reduce function outputs must be of the same type as its input pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JL7V53N_gkeX"
   },
   "source": [
    "## 7. Some more MapReduce and SQL\n",
    "\n",
    "Design, in Python or pseudo-code, MapReduce functions that take a very large file of integers and produce as output:\n",
    "\n",
    "1. The largest integer.\n",
    "1. The average of all the integers.\n",
    "1. The set of integers, but with each integer appearing only once.\n",
    "1. The number of times each unique integer appears.\n",
    "1. The number of distinct integers in the input.\n",
    "\n",
    "For each of these, write the equivalent SQL query, assuming you have a column `values` that stores all the integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QLA_bj1gkeY"
   },
   "source": [
    "## 8. TF-IDF in MapReduce (optional)\n",
    "Imagine we want to build a search engine over the Gutenberg dataset of ~3000 books. Given a word or a set of words, we want to rank these books according their relevance for these words. We need a metric to measure the importance of a word in a set of document...\n",
    "\n",
    "### 8.1 Understand TF-IDF\n",
    "\n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a statistic to determine the relative importance of the words in a set of documents. It is computed as the product of two statistics, term frequency (`tf`) and inverse document frequency (`idf`). \n",
    "\n",
    "Given a word `t`, a document `d` (in this case a book) and the collection of all documents `D`, we can define `tf(t, d)` as the number of times `t` appears in `d`. This gives us some information about the content of a document but because some terms (eg. \"the\") are so common, term frequency will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms.\n",
    "\n",
    "The inverse document frequency `idf(t, D)` is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It can be computed as:\n",
    "\n",
    "![idf](./figures/idf.png)\n",
    "\n",
    "where $|D|$ is the total number of documents and the denominator represents how many documents contain the word $t$ at least once. However, this would cause a division-by-zero exception if the user query a word that never appear in the dataset. A better formulation would be:\n",
    "\n",
    "![idfs](./figures/idf_smooth.png)\n",
    "\n",
    "Then, the `tdidf(t, d, D)` is calculated as follows:\n",
    "\n",
    "![tdidf](./figures/tfidf.png)\n",
    "\n",
    "A high weight in `tfidf` is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents.\n",
    "\n",
    "### 8.2 Implement TF-IDF in MapReduce (pseudo-code)\n",
    "Implement Mapper and Reducer functions in pseudo-code to compute TF-IDF. Assume each Mapper receives the document name as string key and the entire document content as string value. The output of your job should be a list of key-value pairs, where the key is a string in the form \"document:word\" and the value is the `tfidf` score for that document/word pair.\n",
    "\n",
    "```js\n",
    "function map(doc_id, doc_content)\n",
    "  ...\n",
    "```\n",
    "\n",
    "```js\n",
    "function reduce(key, values[])\n",
    "  ...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Exercise07_MapReduce_Solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
