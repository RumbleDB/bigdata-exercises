{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Big Data &ndash; Exercises</center>\n",
    "## <center>Fall 2021 &ndash; Week 9 &ndash; ETH Zurich</center>\n",
    "## <center>Spark Dataframes and SparkSQL</center>\n",
    "\n",
    "# Preparation for the exercise in Spark\n",
    "\n",
    "1. Change to exercise09 repository\n",
    "\n",
    "2. Start docker <br>\n",
    "```docker-compose up -d```\n",
    "\n",
    "3. Copy the data to the same directory as the `docker_compose.yml` in the exam magic box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>1. Spark Dataframes</center>\n",
    "\n",
    "Spark Dataframes allow the user to perform simple and efficient operations on data, as long as the data is structured and has a schema. Dataframes are similar to relational tables in relational databases: conceptually a dataframe is a specialization of a Spark RDD with schema information attached. You can find more information in Karau, H. et al. (2015). Learning Spark, Chapter 9 (optional reading)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3314.6728515625,
      "end_time": 1573739117708.542
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "path = \"orders.jsonl\"\n",
    "orders_df = spark.read.json(path).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of our dataset object is DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 35.862060546875,
      "end_time": 1573665101742.127
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.81103515625,
      "end_time": 1573665103317.247
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- price: double (nullable = true)\n",
      " |    |    |-- product: string (nullable = true)\n",
      " |    |    |-- quantity: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3283.30908203125,
      "end_time": 1573665107643.345
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(customer=Row(first_name='Preston', last_name='Landry'), date='2018-2-4', items=[Row(price=1.53, product='fan', quantity=5), Row(price=1.33, product='computer screen', quantity=6), Row(price=1.06, product='kettle', quantity=6), Row(price=1.96, product='stuffed animal', quantity=3), Row(price=1.09, product='the book', quantity=7), Row(price=1.42, product='headphones', quantity=9), Row(price=1.67, product='whiskey bottle', quantity=3)], order_id=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.limit(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the underlying RDD object and use any functions you learned for Spark RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 17329.39501953125,
      "end_time": 1573665345486.969
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1960"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.rdd.filter(lambda ordr: ordr.customer.last_name == \"Landry\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dataframe Operations\n",
    "We perform some queries using operations on Dataframes ([Here](https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#untyped-dataset-operations-aka-dataframe-operations) is a guide on DF Operations with a link to the [API Documentation](https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select columns and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 252.5771484375,
      "end_time": 1573665989686.293
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|   Preston|   Landry|\n",
      "|    Jamari|Dominguez|\n",
      "|   Brendon|  Sicilia|\n",
      "|    Armani|   Ardeni|\n",
      "|    Jamari|     Miao|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.select(\"customer.first_name\", \"customer.last_name\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we can navigate to the nested items with the dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2263.60888671875,
      "end_time": 1573665774856.528
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1960"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.filter(orders_df[\"customer.last_name\"] == \"Landry\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about nested arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.12890625,
      "end_time": 1573666229796.764
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|order_id|               items|\n",
      "+--------+--------------------+\n",
      "|       0|[{1.53, fan, 5}, ...|\n",
      "|       1|[{1.61, fan, 7}, ...|\n",
      "|       2|[{1.41, the book,...|\n",
      "|       3|[{1.05, computer ...|\n",
      "|       4|[{1.92, headphone...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.select(\"order_id\", \"items\").orderBy(\"order_id\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to find orders of a fan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 239.119140625,
      "end_time": 1573666737735.271
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(items.`product` = 'fan')' due to data type mismatch: differing types in '(items.`product` = 'fan')' (array<string> and string).;\n'Filter (items#9.product = fan)\n+- Relation [customer#7,date#8,items#9,order_id#10L] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66/1552950720.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morders_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morders_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"items.product\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1733\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1734\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(items.`product` = 'fan')' due to data type mismatch: differing types in '(items.`product` = 'fan')' (array<string> and string).;\n'Filter (items#9.product = fan)\n+- Relation [customer#7,date#8,items#9,order_id#10L] json\n"
     ]
    }
   ],
   "source": [
    "orders_df.filter(orders_df[\"items.product\"] == \"fan\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code doesn't work! Use ```array contains``` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 251.64599609375,
      "end_time": 1573666726393.938
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32778"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "orders_df.filter(array_contains(\"items.product\", \"fan\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to unnest the data.\n",
    "\n",
    "Unnest the products with explode.\n",
    "\n",
    "Explode will generate as many rows as there are elements in the array and match them to other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1255.80712890625,
      "end_time": 1573666787807.612
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------+\n",
      "|                   i|        product|order_id|\n",
      "+--------------------+---------------+--------+\n",
      "|      {1.53, fan, 5}|            fan|       0|\n",
      "|{1.33, computer s...|computer screen|       0|\n",
      "|   {1.06, kettle, 6}|         kettle|       0|\n",
      "|{1.96, stuffed an...| stuffed animal|       0|\n",
      "| {1.09, the book, 7}|       the book|       0|\n",
      "+--------------------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\").orderBy(\"order_id\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this table to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 746.837158203125,
      "end_time": 1573667003917.751
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39922"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\")\n",
    "exploded_df.filter(exploded_df[\"product\"] == \"fan\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have tried to access the i.product column directly using a ```.filter``` right after the ```.select```. That, however, does not work, because the column is not available to ```orders_df``` when creating a clause like ```(orders_df[\"i.product\"] == \"fan\")```. A possible workaround when using Dataframe operations is that of using a string clause in ```.filter```, so that the product column will be resolved after it has been added with the ```.select```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 247.906005859375,
      "end_time": 1573667777707.59
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39922"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.select(explode(\"items\").alias(\"i\"), \"i.product\", \"order_id\").filter(\"product = 'fan'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the nested columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 269.365966796875,
      "end_time": 1573669285846.051
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+--------+-----+---------------+--------+\n",
      "|order_id|first_name|last_name|    date|price|        product|quantity|\n",
      "+--------+----------+---------+--------+-----+---------------+--------+\n",
      "|       0|   Preston|   Landry|2018-2-4| 1.53|            fan|       5|\n",
      "|       0|   Preston|   Landry|2018-2-4| 1.33|computer screen|       6|\n",
      "|       0|   Preston|   Landry|2018-2-4| 1.06|         kettle|       6|\n",
      "+--------+----------+---------+--------+-----+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.select(explode(\"items\").alias(\"i\"), \"*\").select(\n",
    "    \"order_id\", \"customer.*\", \"date\", \"i.*\").limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Find the average quantity at which each product is purchased. Only show the top 10 products by quantity. (Hint: you may need to import the function ```desc``` from ```pyspark.sql.functions``` to define descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 4287.89892578125,
      "end_time": 1573675535490.617
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|        product|    avg(quantity)|\n",
      "+---------------+-----------------+\n",
      "|        toaster|5.515549016184942|\n",
      "|       the book|5.514178678641427|\n",
      "|         kettle|5.512053325314489|\n",
      "|computer screen|5.504839685420448|\n",
      "|     mouse trap|5.503895651308093|\n",
      "|            fan|5.496342868593758|\n",
      "|     headphones|5.485920795060985|\n",
      "|       notebook|5.483182341458532|\n",
      "| whiskey bottle|5.475555222463714|\n",
      "| stuffed animal|5.470854598218753|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "orders_df.select(explode(\"items\").alias(\"i\"), \"*\").select(\n",
    "    \"i.product\", \"i.quantity\"\n",
    ").groupBy(\"product\").avg(\"quantity\").orderBy(desc(\"avg(quantity)\")).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the most expensive order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2290.953125,
      "end_time": 1573669705281.358
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|order_id|        sum(total)|\n",
      "+--------+------------------+\n",
      "|   99636|104.95999999999998|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_df = orders_df.select(explode(\"items\").alias(\"i\"), \"*\")\n",
    "exploded_df.select(\n",
    "    \"order_id\", (exploded_df[\"i.quantity\"] * exploded_df[\"i.price\"]).alias(\"total\")\n",
    ").groupBy(\"order_id\").sum(\"total\").orderBy(desc(\"sum(total)\")).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>2. Spark SQL</center>\n",
    "\n",
    "Spark SQL allows the users to formulate their queries using SQL. The requirement is the use of Dataframes, which as said before are similar to relational tables. In addition to a familiar interface, writing queries in SQL might provide better performance than RDDs, inheriting efficiency from the Dataframe operations, while also performing automatic optimization of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install the sparksql magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sparksql-magic in /usr/local/lib/python3.7/dist-packages (0.0.3)\n",
      "Requirement already satisfied: pyspark>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from sparksql-magic) (3.2.0)\n",
      "Requirement already satisfied: ipython>=7.4.0 in /usr/local/lib/python3.7/dist-packages (from sparksql-magic) (7.34.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython>=7.4.0->sparksql-magic) (40.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.19.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (5.9.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (3.0.39)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (2.16.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (4.8.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark>=2.3.0->sparksql-magic) (0.10.9.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.4.0->sparksql-magic) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.4.0->sparksql-magic) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.4.0->sparksql-magic) (0.2.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sparksql-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use sql we need to create a temporary table.\n",
    "\n",
    "This table only exists for the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 42.614990234375,
      "end_time": 1573668230627.757
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run SQL queries on the registered tables. We will run the same queries as during the previous section, but with SQL.\n",
    "\n",
    "As you can see we can navigate to the nested items with the dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 11478.6259765625,
      "end_time": 1573665795839.541
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>1960</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- Finally, run SQL queries on the registered tables\n",
    "-- As you can see we can navigate to the nested items with the dot\n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE orders.customer.last_name == \"Landry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about nested arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2276.55419921875,
      "end_time": 1573666251672.414
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">items</td></tr><tr><td>0</td><td>[Row(price=1.53, product=&#x27;fan&#x27;, quantity=5), Row(price=1.33, product=&#x27;computer screen&#x27;, quantity=6), Row(price=1.06, product=&#x27;kettle&#x27;, quantity=6), Row(price=1.96, product=&#x27;stuffed animal&#x27;, quantity=3), Row(price=1.09, product=&#x27;the book&#x27;, quantity=7), Row(price=1.42, product=&#x27;headphones&#x27;, quantity=9), Row(price=1.67, product=&#x27;whiskey bottle&#x27;, quantity=3)]</td></tr><tr><td>1</td><td>[Row(price=1.61, product=&#x27;fan&#x27;, quantity=7), Row(price=1.39, product=&#x27;whiskey bottle&#x27;, quantity=2)]</td></tr><tr><td>2</td><td>[Row(price=1.41, product=&#x27;the book&#x27;, quantity=6), Row(price=1.3, product=&#x27;notebook&#x27;, quantity=5), Row(price=1.1, product=&#x27;fan&#x27;, quantity=7), Row(price=1.5, product=&#x27;stuffed animal&#x27;, quantity=10), Row(price=1.39, product=&#x27;headphones&#x27;, quantity=8), Row(price=1.78, product=&#x27;whiskey bottle&#x27;, quantity=3), Row(price=1.15, product=&#x27;fan&#x27;, quantity=8)]</td></tr><tr><td>3</td><td>[Row(price=1.05, product=&#x27;computer screen&#x27;, quantity=10), Row(price=1.5, product=&#x27;stuffed animal&#x27;, quantity=10), Row(price=1.42, product=&#x27;whiskey bottle&#x27;, quantity=10)]</td></tr><tr><td>4</td><td>[Row(price=1.92, product=&#x27;headphones&#x27;, quantity=2), Row(price=1.44, product=&#x27;fan&#x27;, quantity=2), Row(price=1.84, product=&#x27;kettle&#x27;, quantity=4), Row(price=1.44, product=&#x27;stuffed animal&#x27;, quantity=5)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- How about nested arrays?\n",
    "SELECT order_id, items\n",
    "FROM orders AS o\n",
    "ORDER BY order_id\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to find orders of a fan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 248.202880859375,
      "end_time": 1573666528302.263
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(orders.items.`product` = 'fan')' due to data type mismatch: differing types in '(orders.items.`product` = 'fan')' (array<string> and string).; line 3 pos 6;\n'Aggregate [unresolvedalias(count(1), None)]\n+- 'Filter (items#9.product = fan)\n   +- SubqueryAlias orders\n      +- View (`orders`, [customer#7,date#8,items#9,order_id#10L])\n         +- Relation [customer#7,date#8,items#9,order_id#10L] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66/461627273.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sparksql'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SELECT count(*)\\nFROM orders\\nWHERE items.product = \"fan\"\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sparksql_magic/sparksql.py\u001b[0m in \u001b[0;36msparksql\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbind_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cache dataframe with %s load'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'eager'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'lazy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(orders.items.`product` = 'fan')' due to data type mismatch: differing types in '(orders.items.`product` = 'fan')' (array<string> and string).; line 3 pos 6;\n'Aggregate [unresolvedalias(count(1), None)]\n+- 'Filter (items#9.product = fan)\n   +- SubqueryAlias orders\n      +- View (`orders`, [customer#7,date#8,items#9,order_id#10L])\n         +- Relation [customer#7,date#8,items#9,order_id#10L] json\n"
     ]
    }
   ],
   "source": [
    "%%sparksql \n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE items.product = \"fan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code doesn't work! Use ```array contains``` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 752.942138671875,
      "end_time": 1573666530734.473
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>32778</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "\n",
    "SELECT count(*)\n",
    "FROM orders\n",
    "WHERE array_contains(items.product, \"fan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to unnest the data.\n",
    "\n",
    "Unnest the products with explode.\n",
    "\n",
    "Explode will generate as many rows as there are elements in the array and match them to other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 772.9169921875,
      "end_time": 1573667016192.464
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">i</td><td style=\"font-weight: bold\">product</td><td style=\"font-weight: bold\">order_id</td></tr><tr><td>Row(price=1.53, product=&#x27;fan&#x27;, quantity=5)</td><td>fan</td><td>0</td></tr><tr><td>Row(price=1.33, product=&#x27;computer screen&#x27;, quantity=6)</td><td>computer screen</td><td>0</td></tr><tr><td>Row(price=1.06, product=&#x27;kettle&#x27;, quantity=6)</td><td>kettle</td><td>0</td></tr><tr><td>Row(price=1.96, product=&#x27;stuffed animal&#x27;, quantity=3)</td><td>stuffed animal</td><td>0</td></tr><tr><td>Row(price=1.09, product=&#x27;the book&#x27;, quantity=7)</td><td>the book</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT explode(items) as i, i.product, order_id\n",
    "FROM orders\n",
    "ORDER BY order_id\n",
    "limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this table to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3281.930908203125,
      "end_time": 1573667022422.047
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>39922</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "-- Filter on product\n",
    "SELECT count(*)\n",
    "    FROM (\n",
    "    SELECT explode(items) as i, i.product, order_id\n",
    "    FROM orders\n",
    "    ORDER BY order_id\n",
    "    )\n",
    "WHERE product = \"fan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have tried to access the i.product column directly in the same ```SELECT``` clause. That, however, does not work, because the column is not available to the ```WHERE``` clause. In order to access the built columns directly, we need to unnest the data and make it part of our ```FROM``` clause. ```LATERAL VIEW``` lets us do just that, matching each non-array attribute to an unnested row from the array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 770.024169921875,
      "end_time": 1573667932258.994
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">customer</td><td style=\"font-weight: bold\">date</td><td style=\"font-weight: bold\">items</td><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">flat_items</td></tr><tr><td>Row(first_name=&#x27;Preston&#x27;, last_name=&#x27;Landry&#x27;)</td><td>2018-2-4</td><td>[Row(price=1.53, product=&#x27;fan&#x27;, quantity=5), Row(price=1.33, product=&#x27;computer screen&#x27;, quantity=6), Row(price=1.06, product=&#x27;kettle&#x27;, quantity=6), Row(price=1.96, product=&#x27;stuffed animal&#x27;, quantity=3), Row(price=1.09, product=&#x27;the book&#x27;, quantity=7), Row(price=1.42, product=&#x27;headphones&#x27;, quantity=9), Row(price=1.67, product=&#x27;whiskey bottle&#x27;, quantity=3)]</td><td>0</td><td>Row(price=1.53, product=&#x27;fan&#x27;, quantity=5)</td></tr><tr><td>Row(first_name=&#x27;Jamari&#x27;, last_name=&#x27;Dominguez&#x27;)</td><td>2016-1-8</td><td>[Row(price=1.61, product=&#x27;fan&#x27;, quantity=7), Row(price=1.39, product=&#x27;whiskey bottle&#x27;, quantity=2)]</td><td>1</td><td>Row(price=1.61, product=&#x27;fan&#x27;, quantity=7)</td></tr><tr><td>Row(first_name=&#x27;Brendon&#x27;, last_name=&#x27;Sicilia&#x27;)</td><td>2016-6-6</td><td>[Row(price=1.41, product=&#x27;the book&#x27;, quantity=6), Row(price=1.3, product=&#x27;notebook&#x27;, quantity=5), Row(price=1.1, product=&#x27;fan&#x27;, quantity=7), Row(price=1.5, product=&#x27;stuffed animal&#x27;, quantity=10), Row(price=1.39, product=&#x27;headphones&#x27;, quantity=8), Row(price=1.78, product=&#x27;whiskey bottle&#x27;, quantity=3), Row(price=1.15, product=&#x27;fan&#x27;, quantity=8)]</td><td>2</td><td>Row(price=1.1, product=&#x27;fan&#x27;, quantity=7)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT *\n",
    "FROM orders lateral view explode(items) as flat_items\n",
    "WHERE flat_items.product = \"fan\"\n",
    "ORDER BY order_id\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the nested columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2275.98095703125,
      "end_time": 1573667943996.512
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">first_name</td><td style=\"font-weight: bold\">last_name</td><td style=\"font-weight: bold\">date</td><td style=\"font-weight: bold\">price</td><td style=\"font-weight: bold\">product</td><td style=\"font-weight: bold\">quantity</td></tr><tr><td>0</td><td>Preston</td><td>Landry</td><td>2018-2-4</td><td>1.53</td><td>fan</td><td>5</td></tr><tr><td>1</td><td>Jamari</td><td>Dominguez</td><td>2016-1-8</td><td>1.61</td><td>fan</td><td>7</td></tr><tr><td>2</td><td>Brendon</td><td>Sicilia</td><td>2016-6-6</td><td>1.1</td><td>fan</td><td>7</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT order_id, customer.first_name, customer.last_name, date, flat_items.*\n",
    "FROM orders lateral view explode(items) item_table as flat_items\n",
    "WHERE flat_items.product = \"fan\"\n",
    "ORDER BY order_id\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built an unnested table, we can now easily aggregate over the previously nested columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Find the average quantity at which each product is purchased. Only show the top 10 products by quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2274.546142578125,
      "end_time": 1573669714658.905
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">product</td><td style=\"font-weight: bold\">av_quantity</td></tr><tr><td>toaster</td><td>5.515549016184942</td></tr><tr><td>the book</td><td>5.514178678641427</td></tr><tr><td>kettle</td><td>5.512053325314489</td></tr><tr><td>computer screen</td><td>5.504839685420448</td></tr><tr><td>mouse trap</td><td>5.503895651308093</td></tr><tr><td>fan</td><td>5.496342868593758</td></tr><tr><td>headphones</td><td>5.485920795060985</td></tr><tr><td>notebook</td><td>5.483182341458532</td></tr><tr><td>whiskey bottle</td><td>5.475555222463714</td></tr><tr><td>stuffed animal</td><td>5.470854598218753</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT flat_items.product, AVG(flat_items.quantity) as av_quantity\n",
    "FROM orders lateral view explode(items) flat_table flat_items\n",
    "GROUP BY flat_items.product\n",
    "ORDER BY av_quantity DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the most expensive order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1268.054931640625,
      "end_time": 1573669716818.317
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">order_id</td><td style=\"font-weight: bold\">total</td></tr><tr><td>99636</td><td>104.95999999999998</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT order_id, SUM(flat_items.quantity * flat_items.price) as total\n",
    "FROM orders lateral view explode(items) flat_table flat_items\n",
    "GROUP BY order_id\n",
    "ORDER BY total desc\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>3. Exercise: PageRank</center>\n",
    "\n",
    "The PageRank algorithm, named after Google's Larry Page, assigns a measure of importance to each node (page) in a graph based on the importance of incoming edges (links). The importance of each edge is, in turn, derived from the importance of the source node and its out-degree. PageRank was designed to rank web pages based on hyperlinks between pages, but it can be also used to rank scientific articles, or influential users in a social network.\n",
    "\n",
    "The algorithm maintains two datasets: one collection of (*pageID*, *linkList*) elements containing the list of neighbors of each page, and one collection of (*pageID*, *rank*) elements containing the current rank for each page. The algorithm proceeds as follows:\n",
    "1. Initialize each page's rank to $1.0$.\n",
    "2. On each iteration, have page $x$ send a contribution of $\\frac{rank(x)}{numNeighbors(x)}$ to its neighbors (the pages it has links to).\n",
    "3. Set each page's rank to $0.15 + 0.85 \\times contributionsReceived$.\n",
    "\n",
    "The algorithm runs multiple iterations (of step 2 and 3) until it converges.\n",
    "\n",
    "Implement the PageRank algorithm in Spark for a simple dataset, running the loop for a fixed number of iterations.\n",
    "\n",
    "For instance, you can use \"parallelize\" for that as follows: \n",
    "```\n",
    "links = sc.parallelize([(1, 2),(1, 4),(2, 1),(2, 3),(3, 2)])\n",
    "```\n",
    "where 1,2,3,4 represents ids of pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Use Spark RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3302.300048828125,
      "end_time": 1573741251342.785
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.7568028566886496),\n",
       " (4, 0.3522676188962165),\n",
       " (1, 0.49149688216717613),\n",
       " (3, 0.49149688216717613)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = sc.parallelize([(1, 2),(1, 4),(2, 1),(2, 3),(3, 2)]).groupByKey().cache()\n",
    "\n",
    "ranks = links.mapValues(lambda x: 1.0)\n",
    "\n",
    "for i in range(10):\n",
    "    contributions = links.join(ranks).flatMap(\n",
    "        lambda xs: [(dest, xs[1][1]/len(list(xs[1][0]))) for dest in list(xs[1][0])]\n",
    "    )\n",
    "    ranks = contributions.reduceByKey(lambda x, y: x + y).mapValues(lambda v: 0.15 + 0.85*v)\n",
    "    \n",
    "ranks.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Use Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 768.98388671875,
      "end_time": 1573749450094.572
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "links = sc.parallelize([(1, 2),(1, 4),(2, 1),(2, 3),(3, 2)])\n",
    "links_df = spark.createDataFrame(links).toDF(\"page_id\", \"linked_id\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 15596.115966796875,
      "end_time": 1573749670841.557
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|page_id|     rank|\n",
      "+-------+---------+\n",
      "|      1|0.5070700|\n",
      "|      3|0.5070700|\n",
      "|      2|0.8035221|\n",
      "|      4|0.3678407|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "links_c = links_df.groupBy(\"page_id\").count()\n",
    "ranks = links_df.selectExpr(\"page_id\", \"1.0 as rank\")\n",
    "\n",
    "for i in range(10):\n",
    "    ranks = ranks.join(links_df, \"page_id\").join(links_c, \"page_id\").selectExpr(\n",
    "        \"*\", \"rank / count as contrib\"\n",
    "    ).groupBy(\"linked_id\").sum(\"contrib\").withColumnRenamed(\"sum(contrib)\", \"sum_contrib\").selectExpr(\n",
    "        \"linked_id as page_id\", \"(0.15 + 0.85*sum_contrib) as rank\"\n",
    "    ).distinct()\n",
    "    \n",
    "ranks.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Use Spark SQL\n",
    "Hint: you can use\n",
    "```\n",
    "new_df = spark.sql(\"... SQL query ...\")\n",
    "new_df.registerTempTable(\"new_table\")\n",
    "```\n",
    "to perform a query inside a for loop and making the updated *new_table* available from SQL at every step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 754.297119140625,
      "end_time": 1573743162150.08
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "links = sc.parallelize([(1, 2),(1, 4),(2, 1),(2, 3),(3, 2)])\n",
    "links_df = spark.createDataFrame(links).toDF(\"page_id\", \"linked_id\").cache()\n",
    "\n",
    "links_df.createOrReplaceTempView(\"links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 19651.471923828125,
      "end_time": 1573746674223.304
     }
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|page_id|    rank|\n",
      "+-------+--------+\n",
      "|      1|0.468064|\n",
      "|      3|0.468064|\n",
      "|      2|0.754215|\n",
      "|      4|0.351405|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate link counts\n",
    "links_c = spark.sql(\"\"\"\n",
    "    SELECT page_id, COUNT(linked_id) AS count_links\n",
    "    FROM links\n",
    "    GROUP BY page_id\n",
    "\"\"\")\n",
    "links_c.createOrReplaceTempView(\"links_c\")\n",
    "\n",
    "# Initialize ranks\n",
    "ranks = spark.sql(\"\"\"\n",
    "    SELECT page_id, 1.0 AS rank\n",
    "    FROM links\n",
    "    GROUP BY page_id\n",
    "\"\"\")\n",
    "ranks.createOrReplaceTempView(\"ranks\")\n",
    "\n",
    "# PageRank algorithm iterations\n",
    "for i in range(15):\n",
    "    new_ranks = spark.sql(\"\"\"\n",
    "        SELECT l.linked_id AS page_id, (0.15 + 0.85 * SUM(r.rank / c.count_links)) AS rank\n",
    "        FROM links l\n",
    "        JOIN ranks r ON l.page_id = r.page_id\n",
    "        JOIN links_c c ON l.page_id = c.page_id\n",
    "        GROUP BY l.linked_id\n",
    "    \"\"\")\n",
    "\n",
    "    # Drop the old 'ranks' view\n",
    "    spark.catalog.dropTempView(\"ranks\")\n",
    "\n",
    "    # Create new 'ranks' view with updated data\n",
    "    new_ranks.createOrReplaceTempView(\"ranks\")\n",
    "\n",
    "# Show final ranks\n",
    "ranks = spark.sql(\"SELECT * FROM ranks\")\n",
    "ranks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
